{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NN, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(3,3)\n",
    "        self.fc2 = torch.nn.Linear(3,2)\n",
    "    def forward(self, x):\n",
    "        x = torch.nn.functional.relu(self.fc1(x))\n",
    "        return self.fc2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.Tensor([[.1,.1,0.],[.1,.1,0],[.1,.1,1],[.1,.1,1]]).float()\n",
    "y = torch.Tensor([0,0,1,1]).long()\n",
    "x.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(4)\n",
    "nn = NN()\n",
    "optimizer = torch.optim.SGD(nn.parameters(), lr=.9)\n",
    "\n",
    "for i in range(50):\n",
    "    output = nn(x)\n",
    "    loss = torch.nn.functional.cross_entropy(output, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_jacobian(net, x, noutputs):\n",
    "    x = x.squeeze()\n",
    "    n = x.size()[0]\n",
    "    x = x.repeat(noutputs, 1)\n",
    "    x.requires_grad_(True)\n",
    "    y = net(x)\n",
    "    y.backward(torch.eye(noutputs))\n",
    "    return x.grad.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3000, -0.1000,  0.0000],\n",
       "        [ 0.3000, -0.1000,  0.0000],\n",
       "        [ 0.5000, -2.0000,  1.0000],\n",
       "        [ 0.5000, -2.0000,  1.0000],\n",
       "        [-0.8000,  2.0000,  1.0000],\n",
       "        [-0.8000,  2.0000,  1.0000]], requires_grad=True)"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_out = 2\n",
    "inp_dims = x_.shape[1]\n",
    "batch_size = 3\n",
    "x_ = torch.Tensor([[.3,-.1,0],\n",
    "                   [.5,-2,1],\n",
    "                   [-.8,2,1]])\n",
    "x_ = x_.repeat(1, n_out).reshape(x_.shape[0]*n_out, x_.shape[1])\n",
    "x_.requires_grad = True\n",
    "x_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.]])"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradients = torch.eye(n_out).repeat(batch_size, 1)\n",
    "gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_ = nn(x_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 4.2701e+01,  4.2685e+01, -5.6636e+02],\n",
       "        [-4.1323e+01, -4.1308e+01,  5.4808e+02],\n",
       "        [ 1.0240e-02,  1.7401e-02,  1.6279e-02],\n",
       "        [-1.0188e-01, -1.7313e-01, -1.6197e-01],\n",
       "        [ 6.2398e-02,  5.2298e-02,  1.5011e-02],\n",
       "        [-1.8815e-01, -1.5770e-01, -4.5265e-02]])"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_.backward(gradients)\n",
    "x_.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 410.3284, -397.2157],\n",
       "        [ 410.3284, -397.2157]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = torch.Tensor([[.1,.05,0],[.1,.05,0]])\n",
    "x1.requires_grad = True\n",
    "y1 = nn(x1)\n",
    "y1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -41.3233,  -41.3075,  548.0845],\n",
       "        [  42.7013,   42.6850, -566.3608]])"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y1.backward(torch.Tensor([[0,1],[1,0]]))\n",
    "x1.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 1])\n",
      "tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 5\n",
    "nb_digits = 10\n",
    "# Dummy input that HAS to be 2D for the scatter (you can use view(-1,1) if needed)\n",
    "y = torch.LongTensor(batch_size,1).random_() % nb_digits\n",
    "# One hot encoding buffer that you create out of the loop and just keep reusing\n",
    "y_onehot = torch.FloatTensor(batch_size, nb_digits)\n",
    "\n",
    "# In your for loop\n",
    "y_onehot.zero_()\n",
    "y_onehot.scatter_(1, y, 1)\n",
    "\n",
    "print(y.shape)\n",
    "print(y_onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_.requires_grad = True\n",
    "output = nn(x_)\n",
    "print(f'{output.data[0]} ==> it\\'s a {output.data[0].max(0)[1].item()}!')\n",
    "# loss = torch.nn.functional.cross_entropy(output, )\n",
    "output.backward(torch.eye(2))\n",
    "out0_grad = torch.autograd.grad(output[0,:].sum(), x_, retain_graph=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
