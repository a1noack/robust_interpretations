{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Code assumes the ability to train using a GPU with CUDA.\n",
    "\"\"\"\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from advertorch.attacks import GradientSignAttack, CarliniWagnerL2Attack, PGDAttack\n",
    "import matplotlib.pyplot as plt\n",
    "import utils.adv_ex_utils as aus\n",
    "from utils.models import LeNet\n",
    "import utils.interp_generators as igs\n",
    "\n",
    "\n",
    "# makes default tensor a CUDA tensor so GPU can be used\n",
    "torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "device = 2\n",
    "torch.cuda.set_device(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_batch_size = 64\n",
    "te_batch_size = 50\n",
    "\n",
    "data_preprocess = torchvision.transforms.Compose([\n",
    "                        torchvision.transforms.ToTensor(),\n",
    "                        torchvision.transforms.Normalize((0.1307,), (0.3081,))])\n",
    "# the mean of mnist pixel data is .1307 and the stddev is .3081\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "                    torchvision.datasets.MNIST('./data', train=True, download=True,\n",
    "                         transform=data_preprocess), \n",
    "                    batch_size=tr_batch_size, \n",
    "                    shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "                    torchvision.datasets.MNIST('./data', train=False, download=True,\n",
    "                         transform=data_preprocess), \n",
    "                    batch_size=te_batch_size, \n",
    "                    shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for Jacobian regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bp_matrix(batch_size, n_outputs):\n",
    "    \"\"\"Creates matrix that is used to calculate Jacobian for multiple input \n",
    "    samples at once.\n",
    "    \"\"\"\n",
    "    idx = torch.arange(n_outputs).reshape(n_outputs,1).repeat(1,batch_size).reshape(batch_size*n_outputs,)\n",
    "    return torch.zeros(len(idx), n_outputs).scatter_(1, idx.unsqueeze(1), 1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_norm_jacobian(x, n_outputs, bp_mat, for_loss=True):\n",
    "    \"\"\"Returns squared frobenius norm of the input-output Jacobian averaged \n",
    "    over the entire batch of inputs in x.\n",
    "    \"\"\"\n",
    "    batch_size = x.shape[0]\n",
    "    # needed because some edge-case batches are not standard size\n",
    "    if bp_mat.shape[0]/n_outputs != batch_size:     \n",
    "        bp_mat = bp_matrix(batch_size, n_outputs)\n",
    "    x = x.repeat(n_outputs, 1, 1, 1)\n",
    "    x = Variable(x, requires_grad=True)\n",
    "    # needed so that we can get gradient of output w.r.t input\n",
    "    y = net(x)\n",
    "    x_grad = torch.autograd.grad(y, x, grad_outputs=bp_mat, create_graph=for_loss)[0]\n",
    "    # get sum of squared values of the gradient values \n",
    "    j = x_grad.pow(2).sum() / batch_size\n",
    "    return j"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for interpretation regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_diff_interp(x, labels, ig=igs.simple_gradient):    \n",
    "    ix = ig(net, x, labels, used=False)\n",
    "    x_ = aus.perturb_randomly(x)\n",
    "    ix_ = ig(net, x_, labels, used=False)\n",
    "    diff = torch.abs(ix-ix_)\n",
    "    norm_diff = torch.norm(diff)\n",
    "    ixs = torch.cat([ix,ix_],dim=0)\n",
    "    return norm_diff, ixs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define loss function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_loss(output, labels, alpha_wd=0, alpha_jr=0, x=None, bp_mat=None, alpha_ir1=0, alpha_ir2=0.0001):\n",
    "    \"\"\"Adds terms for L2-regularization and the norm of the input-output \n",
    "    Jacobian to the standard cross-entropy loss function. Check https://arxiv.org/abs/1908.02729\n",
    "    for alpha_wd, alpha_jr suggestions.\n",
    "    \"\"\"\n",
    "    # standard cross-entropy loss base\n",
    "    loss = F.cross_entropy(output, labels)\n",
    "    \n",
    "    # add l2 regularization to loss \n",
    "    if alpha_wd != 0:\n",
    "        l2 = 0\n",
    "        for p in net.parameters():\n",
    "            l2 += p.pow(2).sum()\n",
    "        loss = loss + alpha_wd * l2\n",
    "    \n",
    "    # add input-output jacobian regularization formulation\n",
    "    if alpha_jr != 0:\n",
    "        n_outputs = output.shape[1]\n",
    "        j = avg_norm_jacobian(x, n_outputs, bp_mat)\n",
    "        loss = loss + (alpha_jr / 2) * j\n",
    "        # needed so gradients don't accumulate in leaf variables when calling loss.backward in train function\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "    # add interpretation regularization\n",
    "    if alpha_ir1 != 0:\n",
    "        norm, ix = norm_diff_interp(x, labels)\n",
    "        loss = loss + alpha_ir1 * norm\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # add l0 interpretation regularization\n",
    "        if alpha_ir2 != 0:\n",
    "            loss = loss + alpha_ir2 * torch.sum(torch.abs(ix / (torch.abs(ix) + .0001)))\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define train and test functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(alpha_wd, alpha_jr, alpha_ir, adversary=None):\n",
    "    net.train()\n",
    "    \n",
    "    for batch_idx, (samples, labels) in enumerate(train_loader):\n",
    "        # sends to GPU, i.e. essentially converts from torch.FloatTensor to torch.cuda.FloatTensor\n",
    "        samples, labels = samples.to(device), labels.to(device)\n",
    "        \n",
    "        # expand dataset with adversarial examples if adversary specified\n",
    "        if adversary != None:\n",
    "            adv_samples, adv_labels = aus.generate_adv_exs(samples, labels, adversary)\n",
    "            samples, labels = torch.cat([samples, adv_samples], 0), torch.cat([labels, adv_labels], 0)\n",
    "                \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = net(samples)\n",
    "        \n",
    "        loss = my_loss(output, labels, alpha_wd=alpha_wd, alpha_jr=alpha_jr, x=samples, bp_mat=tr, alpha_ir1=alpha_ir)\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % log_interval == 0:\n",
    "            j = avg_norm_jacobian(samples, output.shape[1], tr, for_loss=False)\n",
    "            i,_ = norm_diff_interp(samples, labels)\n",
    "            print(f'\\tLoss: {loss.item():.6f} Average norm of Jacobian: {j:6f} Norm of difference in interpretations: {i:6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(alpha_wd, alpha_jr, alpha_ir):\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    for samples, labels in test_loader:\n",
    "        samples, labels = samples.to(device), labels.to(device)\n",
    "        output = net(samples)\n",
    "        test_loss += my_loss(output, labels, alpha_wd=alpha_wd, alpha_jr=alpha_jr, x=samples, bp_mat=te, alpha_ir1=alpha_ir).item()\n",
    "        # output is a tensor, .data retrieves its data, max returns the index of the highest valued element\n",
    "        preds = output.data.max(1, keepdim=True)[1]\n",
    "        correct += preds.eq(labels.data.view_as(preds)).sum().item()\n",
    "                \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_accuracy = 100. * float(correct / len(test_loader.dataset))\n",
    "    \n",
    "    print(f'\\tTest set accuracy: ({test_accuracy:.2f}%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with interpretation regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #1\n",
      "\tLoss: 4.847697 Average norm of Jacobian: 0.015538 Norm of difference in interpretations: 0.005444\n",
      "\tLoss: 2.661771 Average norm of Jacobian: 17.968513 Norm of difference in interpretations: 0.004571\n",
      "\tLoss: 2.449969 Average norm of Jacobian: 27.894032 Norm of difference in interpretations: 0.003753\n",
      "\tLoss: 2.397987 Average norm of Jacobian: 45.971424 Norm of difference in interpretations: 0.003798\n",
      "\tLoss: 2.159640 Average norm of Jacobian: 43.929634 Norm of difference in interpretations: 0.003663\n",
      "\tTest set accuracy: (96.71%)\n",
      "Epoch #2\n",
      "\tLoss: 2.172888 Average norm of Jacobian: 47.361488 Norm of difference in interpretations: 0.003795\n",
      "\tLoss: 2.308616 Average norm of Jacobian: 60.109856 Norm of difference in interpretations: 0.003741\n",
      "\tLoss: 2.121992 Average norm of Jacobian: 51.011551 Norm of difference in interpretations: 0.003691\n",
      "\tLoss: 2.336782 Average norm of Jacobian: 40.847168 Norm of difference in interpretations: 0.003297\n",
      "\tLoss: 2.037063 Average norm of Jacobian: 37.391800 Norm of difference in interpretations: 0.003360\n",
      "\tTest set accuracy: (97.22%)\n",
      "Epoch #3\n",
      "\tLoss: 2.140929 Average norm of Jacobian: 61.630714 Norm of difference in interpretations: 0.002940\n",
      "\tLoss: 2.098212 Average norm of Jacobian: 50.761414 Norm of difference in interpretations: 0.003870\n",
      "\tLoss: 2.002402 Average norm of Jacobian: 49.034378 Norm of difference in interpretations: 0.003934\n",
      "\tLoss: 2.077383 Average norm of Jacobian: 37.839554 Norm of difference in interpretations: 0.003258\n",
      "\tLoss: 2.108152 Average norm of Jacobian: 57.097137 Norm of difference in interpretations: 0.003227\n",
      "\tTest set accuracy: (97.96%)\n",
      "Epoch #4\n",
      "\tLoss: 2.147427 Average norm of Jacobian: 45.934601 Norm of difference in interpretations: 0.003640\n",
      "\tLoss: 2.028077 Average norm of Jacobian: 56.356270 Norm of difference in interpretations: 0.003055\n",
      "\tLoss: 2.155833 Average norm of Jacobian: 48.135109 Norm of difference in interpretations: 0.003698\n",
      "\tLoss: 1.906873 Average norm of Jacobian: 64.190544 Norm of difference in interpretations: 0.003078\n",
      "\tLoss: 1.876143 Average norm of Jacobian: 65.500000 Norm of difference in interpretations: 0.003451\n",
      "\tTest set accuracy: (98.06%)\n",
      "Epoch #5\n",
      "\tLoss: 2.041315 Average norm of Jacobian: 40.112160 Norm of difference in interpretations: 0.003536\n",
      "\tLoss: 1.875038 Average norm of Jacobian: 62.792656 Norm of difference in interpretations: 0.003950\n",
      "\tLoss: 2.193053 Average norm of Jacobian: 90.752121 Norm of difference in interpretations: 0.003079\n",
      "\tLoss: 1.966928 Average norm of Jacobian: 57.736450 Norm of difference in interpretations: 0.002857\n",
      "\tLoss: 1.824635 Average norm of Jacobian: 62.982414 Norm of difference in interpretations: 0.005106\n",
      "\tTest set accuracy: (97.35%)\n",
      "Epoch #6\n",
      "\tLoss: 2.012374 Average norm of Jacobian: 57.771599 Norm of difference in interpretations: 0.002778\n",
      "\tLoss: 2.089614 Average norm of Jacobian: 61.648911 Norm of difference in interpretations: 0.003182\n",
      "\tLoss: 1.780624 Average norm of Jacobian: 64.932449 Norm of difference in interpretations: 0.004379\n",
      "\tLoss: 1.992990 Average norm of Jacobian: 48.992020 Norm of difference in interpretations: 0.003830\n",
      "\tLoss: 1.949217 Average norm of Jacobian: 59.819176 Norm of difference in interpretations: 0.003944\n",
      "\tTest set accuracy: (98.25%)\n",
      "Epoch #7\n",
      "\tLoss: 1.869578 Average norm of Jacobian: 39.962589 Norm of difference in interpretations: 0.003881\n",
      "\tLoss: 1.863918 Average norm of Jacobian: 77.385666 Norm of difference in interpretations: 0.002998\n",
      "\tLoss: 2.111912 Average norm of Jacobian: 43.955807 Norm of difference in interpretations: 0.003676\n",
      "\tLoss: 1.979117 Average norm of Jacobian: 59.545502 Norm of difference in interpretations: 0.003075\n",
      "\tLoss: 1.857570 Average norm of Jacobian: 70.803040 Norm of difference in interpretations: 0.003220\n",
      "\tTest set accuracy: (97.95%)\n",
      "Epoch #8\n",
      "\tLoss: 1.932322 Average norm of Jacobian: 66.978928 Norm of difference in interpretations: 0.003995\n",
      "\tLoss: 2.191768 Average norm of Jacobian: 53.016548 Norm of difference in interpretations: 0.003555\n",
      "\tLoss: 1.829037 Average norm of Jacobian: 62.155350 Norm of difference in interpretations: 0.003151\n",
      "\tLoss: 2.086078 Average norm of Jacobian: 70.995560 Norm of difference in interpretations: 0.003398\n",
      "\tLoss: 1.819127 Average norm of Jacobian: 52.468513 Norm of difference in interpretations: 0.003531\n",
      "\tTest set accuracy: (98.12%)\n",
      "Epoch #9\n",
      "\tLoss: 1.955778 Average norm of Jacobian: 51.602921 Norm of difference in interpretations: 0.004187\n",
      "\tLoss: 2.161675 Average norm of Jacobian: 57.061661 Norm of difference in interpretations: 0.003461\n",
      "\tLoss: 2.038066 Average norm of Jacobian: 59.546562 Norm of difference in interpretations: 0.003561\n",
      "\tLoss: 1.910555 Average norm of Jacobian: 64.260590 Norm of difference in interpretations: 0.004003\n",
      "\tLoss: 2.054771 Average norm of Jacobian: 57.718704 Norm of difference in interpretations: 0.003989\n",
      "\tTest set accuracy: (98.10%)\n",
      "Epoch #10\n",
      "\tLoss: 2.052590 Average norm of Jacobian: 64.340027 Norm of difference in interpretations: 0.003561\n",
      "\tLoss: 1.851385 Average norm of Jacobian: 81.146088 Norm of difference in interpretations: 0.003949\n",
      "\tLoss: 1.951775 Average norm of Jacobian: 51.318977 Norm of difference in interpretations: 0.003491\n",
      "\tLoss: 1.959081 Average norm of Jacobian: 56.259636 Norm of difference in interpretations: 0.004378\n",
      "\tLoss: 2.211171 Average norm of Jacobian: 39.721497 Norm of difference in interpretations: 0.003800\n",
      "\tTest set accuracy: (97.51%)\n",
      "Epoch #11\n",
      "\tLoss: 1.959104 Average norm of Jacobian: 91.981537 Norm of difference in interpretations: 0.003698\n",
      "\tLoss: 1.946851 Average norm of Jacobian: 52.387615 Norm of difference in interpretations: 0.003862\n",
      "\tLoss: 1.938480 Average norm of Jacobian: 62.717018 Norm of difference in interpretations: 0.003384\n",
      "\tLoss: 1.941504 Average norm of Jacobian: 45.598450 Norm of difference in interpretations: 0.003808\n",
      "\tLoss: 1.835595 Average norm of Jacobian: 53.445900 Norm of difference in interpretations: 0.004027\n",
      "\tTest set accuracy: (98.40%)\n",
      "Epoch #12\n",
      "\tLoss: 2.197576 Average norm of Jacobian: 54.727345 Norm of difference in interpretations: 0.003203\n",
      "\tLoss: 1.806541 Average norm of Jacobian: 53.547966 Norm of difference in interpretations: 0.002850\n",
      "\tLoss: 1.823839 Average norm of Jacobian: 45.827522 Norm of difference in interpretations: 0.004063\n",
      "\tLoss: 1.974051 Average norm of Jacobian: 53.998047 Norm of difference in interpretations: 0.003478\n",
      "\tLoss: 2.143590 Average norm of Jacobian: 37.474129 Norm of difference in interpretations: 0.003557\n",
      "\tTest set accuracy: (98.50%)\n",
      "Epoch #13\n",
      "\tLoss: 1.905004 Average norm of Jacobian: 50.343071 Norm of difference in interpretations: 0.003419\n",
      "\tLoss: 1.907451 Average norm of Jacobian: 51.302856 Norm of difference in interpretations: 0.003334\n",
      "\tLoss: 1.816918 Average norm of Jacobian: 42.493172 Norm of difference in interpretations: 0.003394\n",
      "\tLoss: 1.736826 Average norm of Jacobian: 57.184475 Norm of difference in interpretations: 0.002910\n",
      "\tLoss: 1.797182 Average norm of Jacobian: 52.512672 Norm of difference in interpretations: 0.003910\n",
      "\tTest set accuracy: (98.42%)\n",
      "Epoch #14\n",
      "\tLoss: 1.697355 Average norm of Jacobian: 72.050659 Norm of difference in interpretations: 0.003085\n",
      "\tLoss: 1.735018 Average norm of Jacobian: 56.862000 Norm of difference in interpretations: 0.003545\n",
      "\tLoss: 1.886599 Average norm of Jacobian: 51.371315 Norm of difference in interpretations: 0.003833\n",
      "\tLoss: 1.973043 Average norm of Jacobian: 72.719086 Norm of difference in interpretations: 0.003593\n",
      "\tLoss: 1.944609 Average norm of Jacobian: 48.198669 Norm of difference in interpretations: 0.003584\n",
      "\tTest set accuracy: (98.37%)\n",
      "Epoch #15\n",
      "\tLoss: 1.795185 Average norm of Jacobian: 50.575569 Norm of difference in interpretations: 0.003857\n",
      "\tLoss: 1.837850 Average norm of Jacobian: 60.001900 Norm of difference in interpretations: 0.003583\n",
      "\tLoss: 1.778343 Average norm of Jacobian: 52.579235 Norm of difference in interpretations: 0.003373\n",
      "\tLoss: 1.837535 Average norm of Jacobian: 60.933640 Norm of difference in interpretations: 0.003631\n",
      "\tLoss: 1.793549 Average norm of Jacobian: 47.660046 Norm of difference in interpretations: 0.003724\n",
      "\tTest set accuracy: (98.47%)\n",
      "Epoch #16\n",
      "\tLoss: 2.129533 Average norm of Jacobian: 48.378082 Norm of difference in interpretations: 0.004055\n",
      "\tLoss: 1.827781 Average norm of Jacobian: 60.563072 Norm of difference in interpretations: 0.003431\n",
      "\tLoss: 1.798234 Average norm of Jacobian: 62.649300 Norm of difference in interpretations: 0.003419\n",
      "\tLoss: 1.992121 Average norm of Jacobian: 57.374100 Norm of difference in interpretations: 0.003912\n",
      "\tLoss: 1.839624 Average norm of Jacobian: 51.067223 Norm of difference in interpretations: 0.003813\n",
      "\tTest set accuracy: (98.49%)\n",
      "Epoch #17\n",
      "\tLoss: 1.929877 Average norm of Jacobian: 67.871109 Norm of difference in interpretations: 0.003603\n",
      "\tLoss: 1.870908 Average norm of Jacobian: 47.013435 Norm of difference in interpretations: 0.003425\n",
      "\tLoss: 1.885447 Average norm of Jacobian: 44.228989 Norm of difference in interpretations: 0.003716\n",
      "\tLoss: 1.972475 Average norm of Jacobian: 41.322968 Norm of difference in interpretations: 0.003800\n",
      "\tLoss: 1.836900 Average norm of Jacobian: 57.834286 Norm of difference in interpretations: 0.003662\n",
      "\tTest set accuracy: (98.44%)\n",
      "Epoch #18\n",
      "\tLoss: 1.813748 Average norm of Jacobian: 51.205200 Norm of difference in interpretations: 0.003929\n",
      "\tLoss: 1.887078 Average norm of Jacobian: 42.277702 Norm of difference in interpretations: 0.003851\n",
      "\tLoss: 2.043390 Average norm of Jacobian: 48.634628 Norm of difference in interpretations: 0.003720\n",
      "\tLoss: 1.974566 Average norm of Jacobian: 55.343575 Norm of difference in interpretations: 0.003683\n",
      "\tLoss: 1.852999 Average norm of Jacobian: 50.110077 Norm of difference in interpretations: 0.003196\n",
      "\tTest set accuracy: (98.46%)\n",
      "Epoch #19\n",
      "\tLoss: 1.915370 Average norm of Jacobian: 58.108803 Norm of difference in interpretations: 0.003451\n",
      "\tLoss: 1.854983 Average norm of Jacobian: 58.802597 Norm of difference in interpretations: 0.003184\n",
      "\tLoss: 1.978430 Average norm of Jacobian: 61.125862 Norm of difference in interpretations: 0.003370\n",
      "\tLoss: 1.719802 Average norm of Jacobian: 39.660179 Norm of difference in interpretations: 0.003729\n",
      "\tLoss: 2.104893 Average norm of Jacobian: 45.317333 Norm of difference in interpretations: 0.003266\n",
      "\tTest set accuracy: (98.46%)\n",
      "Epoch #20\n",
      "\tLoss: 1.910259 Average norm of Jacobian: 54.568893 Norm of difference in interpretations: 0.004549\n",
      "\tLoss: 1.917071 Average norm of Jacobian: 53.123138 Norm of difference in interpretations: 0.003196\n",
      "\tLoss: 1.924557 Average norm of Jacobian: 53.989891 Norm of difference in interpretations: 0.004371\n",
      "\tLoss: 2.055981 Average norm of Jacobian: 50.780357 Norm of difference in interpretations: 0.003679\n",
      "\tLoss: 1.644435 Average norm of Jacobian: 60.819275 Norm of difference in interpretations: 0.002777\n",
      "\tTest set accuracy: (98.51%)\n",
      "Epoch #21\n",
      "\tLoss: 1.757163 Average norm of Jacobian: 56.932877 Norm of difference in interpretations: 0.003348\n",
      "\tLoss: 1.858849 Average norm of Jacobian: 49.740555 Norm of difference in interpretations: 0.003622\n",
      "\tLoss: 1.884658 Average norm of Jacobian: 46.945663 Norm of difference in interpretations: 0.003290\n",
      "\tLoss: 1.903815 Average norm of Jacobian: 40.784447 Norm of difference in interpretations: 0.003760\n",
      "\tLoss: 1.726787 Average norm of Jacobian: 53.833874 Norm of difference in interpretations: 0.003408\n",
      "\tTest set accuracy: (98.50%)\n",
      "Epoch #22\n",
      "\tLoss: 1.744904 Average norm of Jacobian: 50.977715 Norm of difference in interpretations: 0.003872\n",
      "\tLoss: 1.923576 Average norm of Jacobian: 47.054581 Norm of difference in interpretations: 0.003944\n",
      "\tLoss: 1.700773 Average norm of Jacobian: 63.387611 Norm of difference in interpretations: 0.003189\n",
      "\tLoss: 2.112678 Average norm of Jacobian: 49.563580 Norm of difference in interpretations: 0.004728\n",
      "\tLoss: 1.879191 Average norm of Jacobian: 52.638618 Norm of difference in interpretations: 0.002780\n",
      "\tTest set accuracy: (98.56%)\n",
      "Epoch #23\n",
      "\tLoss: 1.954519 Average norm of Jacobian: 45.649670 Norm of difference in interpretations: 0.003546\n",
      "\tLoss: 2.049047 Average norm of Jacobian: 53.497757 Norm of difference in interpretations: 0.003094\n",
      "\tLoss: 1.746451 Average norm of Jacobian: 57.371929 Norm of difference in interpretations: 0.003250\n",
      "\tLoss: 1.688054 Average norm of Jacobian: 52.260399 Norm of difference in interpretations: 0.003398\n",
      "\tLoss: 1.959597 Average norm of Jacobian: 37.374115 Norm of difference in interpretations: 0.004913\n",
      "\tTest set accuracy: (98.54%)\n",
      "Epoch #24\n",
      "\tLoss: 1.662065 Average norm of Jacobian: 51.289238 Norm of difference in interpretations: 0.004121\n",
      "\tLoss: 2.157280 Average norm of Jacobian: 47.093666 Norm of difference in interpretations: 0.003632\n",
      "\tLoss: 1.620918 Average norm of Jacobian: 62.944061 Norm of difference in interpretations: 0.003584\n",
      "\tLoss: 1.927541 Average norm of Jacobian: 46.713982 Norm of difference in interpretations: 0.002964\n",
      "\tLoss: 1.791629 Average norm of Jacobian: 52.234367 Norm of difference in interpretations: 0.003760\n",
      "\tTest set accuracy: (98.58%)\n",
      "Epoch #25\n",
      "\tLoss: 2.153723 Average norm of Jacobian: 39.310345 Norm of difference in interpretations: 0.004199\n",
      "\tLoss: 1.877452 Average norm of Jacobian: 51.707657 Norm of difference in interpretations: 0.004193\n",
      "\tLoss: 1.805180 Average norm of Jacobian: 54.847187 Norm of difference in interpretations: 0.003186\n",
      "\tLoss: 1.829567 Average norm of Jacobian: 47.010731 Norm of difference in interpretations: 0.003825\n",
      "\tLoss: 1.823545 Average norm of Jacobian: 62.661980 Norm of difference in interpretations: 0.003342\n",
      "\tTest set accuracy: (98.55%)\n",
      "Epoch #26\n",
      "\tLoss: 1.783541 Average norm of Jacobian: 51.735844 Norm of difference in interpretations: 0.004001\n",
      "\tLoss: 1.911900 Average norm of Jacobian: 53.612144 Norm of difference in interpretations: 0.003226\n",
      "\tLoss: 1.767389 Average norm of Jacobian: 53.245491 Norm of difference in interpretations: 0.003710\n",
      "\tLoss: 2.106033 Average norm of Jacobian: 65.056625 Norm of difference in interpretations: 0.003415\n",
      "\tLoss: 1.887810 Average norm of Jacobian: 62.799500 Norm of difference in interpretations: 0.003868\n",
      "\tTest set accuracy: (98.45%)\n",
      "Epoch #27\n",
      "\tLoss: 1.823375 Average norm of Jacobian: 53.601738 Norm of difference in interpretations: 0.003964\n",
      "\tLoss: 1.947964 Average norm of Jacobian: 42.959412 Norm of difference in interpretations: 0.003796\n",
      "\tLoss: 1.797992 Average norm of Jacobian: 47.183403 Norm of difference in interpretations: 0.003679\n",
      "\tLoss: 1.965846 Average norm of Jacobian: 49.906509 Norm of difference in interpretations: 0.003229\n",
      "\tLoss: 1.923885 Average norm of Jacobian: 50.362164 Norm of difference in interpretations: 0.004188\n",
      "\tTest set accuracy: (98.49%)\n",
      "Epoch #28\n",
      "\tLoss: 1.814834 Average norm of Jacobian: 59.721886 Norm of difference in interpretations: 0.003924\n",
      "\tLoss: 1.838520 Average norm of Jacobian: 64.994019 Norm of difference in interpretations: 0.003551\n",
      "\tLoss: 1.916360 Average norm of Jacobian: 45.006927 Norm of difference in interpretations: 0.004709\n",
      "\tLoss: 1.809243 Average norm of Jacobian: 57.767414 Norm of difference in interpretations: 0.002257\n",
      "\tLoss: 1.800602 Average norm of Jacobian: 55.320099 Norm of difference in interpretations: 0.003874\n",
      "\tTest set accuracy: (98.50%)\n",
      "Epoch #29\n",
      "\tLoss: 1.830550 Average norm of Jacobian: 59.709656 Norm of difference in interpretations: 0.003469\n",
      "\tLoss: 1.800467 Average norm of Jacobian: 53.710247 Norm of difference in interpretations: 0.003685\n",
      "\tLoss: 1.799194 Average norm of Jacobian: 48.498829 Norm of difference in interpretations: 0.003496\n",
      "\tLoss: 1.956400 Average norm of Jacobian: 53.520340 Norm of difference in interpretations: 0.003465\n",
      "\tLoss: 1.853162 Average norm of Jacobian: 51.299313 Norm of difference in interpretations: 0.003783\n",
      "\tTest set accuracy: (98.52%)\n",
      "Epoch #30\n",
      "\tLoss: 1.873741 Average norm of Jacobian: 57.293724 Norm of difference in interpretations: 0.002992\n",
      "\tLoss: 1.822792 Average norm of Jacobian: 51.025322 Norm of difference in interpretations: 0.004686\n",
      "\tLoss: 2.008389 Average norm of Jacobian: 44.312790 Norm of difference in interpretations: 0.003891\n",
      "\tLoss: 1.881350 Average norm of Jacobian: 58.618565 Norm of difference in interpretations: 0.003393\n",
      "\tLoss: 1.856722 Average norm of Jacobian: 54.393806 Norm of difference in interpretations: 0.002939\n",
      "\tTest set accuracy: (98.51%)\n"
     ]
    }
   ],
   "source": [
    "# training details\n",
    "n_epochs = 30\n",
    "log_interval = 200\n",
    "training_round = 2\n",
    "torch.manual_seed(training_round)\n",
    "\n",
    "# instantiate model and optimizer\n",
    "learning_rate = 0.01\n",
    "momentum = 0.9\n",
    "net = LeNet()\n",
    "optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=momentum)\n",
    "lr_decayer = StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "# make model CUDA enabled and define GPU/device to use\n",
    "net.cuda()\n",
    "\n",
    "tr = bp_matrix(tr_batch_size, 10)\n",
    "te = bp_matrix(te_batch_size, 10)\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    print(f'Epoch #{epoch}')\n",
    "    train(0,0.001,200)\n",
    "    test(0,.001,200)\n",
    "    lr_decayer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), f'trained_models/interp_reg_tests/jr{}_ir1{}_ir2{}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
