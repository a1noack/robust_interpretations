{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NN, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(3,3)\n",
    "        self.fc2 = torch.nn.Linear(3,2)\n",
    "    def forward(self, x):\n",
    "        x = torch.nn.functional.relu(self.fc1(x))\n",
    "        return self.fc2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.Tensor([[.1,.1,0.],[.1,.1,0],[.1,.1,1],[.1,.1,1]]).float()\n",
    "y = torch.Tensor([0,0,1,1]).long()\n",
    "x.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(4)\n",
    "nn = NN()\n",
    "optimizer = torch.optim.SGD(nn.parameters(), lr=.9)\n",
    "\n",
    "for i in range(50):\n",
    "    output = nn(x)\n",
    "    loss = torch.nn.functional.cross_entropy(output, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3000, -0.1000,  0.0000],\n",
       "        [ 0.5000, -2.0000,  1.0000],\n",
       "        [-0.8000,  2.0000,  1.0000],\n",
       "        [ 0.3000, -0.1000,  0.0000],\n",
       "        [ 0.5000, -2.0000,  1.0000],\n",
       "        [-0.8000,  2.0000,  1.0000]], requires_grad=True)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_out = 2\n",
    "batch_size = 3\n",
    "x_ = torch.Tensor([[.3,-.1,0],\n",
    "                   [.5,-2,1],\n",
    "                   [-.8,2,1]])\n",
    "# x_ = x_.repeat(1, n_out).reshape(x_.shape[0]*n_out, x_.shape[1])\n",
    "x_ = x_.repeat(n_out, 1)\n",
    "x_.requires_grad = True\n",
    "x_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.]])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gradients = torch.eye(n_out).repeat(batch_size, 1)\n",
    "idx = torch.arange(n_out).reshape(n_out,1).repeat(1,batch_size).reshape(batch_size*n_out,)\n",
    "gradients = torch.zeros(len(idx), n_out).scatter_(1, idx.unsqueeze(1), 1.)\n",
    "gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 4.2701e+01,  4.2685e+01, -5.6636e+02],\n",
       "        [ 1.0240e-02,  1.7401e-02,  1.6279e-02],\n",
       "        [ 6.2398e-02,  5.2298e-02,  1.5011e-02],\n",
       "        [-4.1323e+01, -4.1308e+01,  5.4808e+02],\n",
       "        [-1.0188e-01, -1.7313e-01, -1.6197e-01],\n",
       "        [-1.8815e-01, -1.5770e-01, -4.5265e-02]], grad_fn=<CloneBackward>)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer.zero_grad()\n",
    "y_ = nn(x_)\n",
    "y_.backward(gradients, create_graph=True)\n",
    "x_.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 4.2701e+01,  4.2685e+01, -5.6636e+02],\n",
       "        [ 1.0240e-02,  1.7401e-02,  1.6279e-02],\n",
       "        [ 6.2398e-02,  5.2298e-02,  1.5011e-02],\n",
       "        [-4.1323e+01, -4.1308e+01,  5.4808e+02],\n",
       "        [-1.0188e-01, -1.7313e-01, -1.6197e-01],\n",
       "        [-1.8815e-01, -1.5770e-01, -4.5265e-02]], grad_fn=<MmBackward>)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer.zero_grad()\n",
    "y_ = nn(x_)\n",
    "torch.autograd.grad(y_, x_, grad_outputs=gradients, create_graph=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "jacobians = x_.grad.reshape(batch_size, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "jacobian_norms = torch.pow(jacobians.norm(p='fro', dim=1), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3.2441e+05, 3.0381e+05, 1.2891e-01], grad_fn=<PowBackward0>)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jacobian_norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 410.3284, -397.2157],\n",
       "        [ 410.3284, -397.2157]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = torch.Tensor([[.1,.05,0],[.1,.05,0]])\n",
    "x1.requires_grad = True\n",
    "y1 = nn(x1)\n",
    "y1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -41.3233,  -41.3075,  548.0845],\n",
       "        [  42.7013,   42.6850, -566.3608]])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y1.backward(torch.Tensor([[0,1],[1,0]]))\n",
    "x1.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 3, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 1.,  2.,  3.],\n",
       "          [ 4.,  5.,  6.],\n",
       "          [ 7.,  8.,  9.]]],\n",
       "\n",
       "\n",
       "        [[[10., 11., 12.],\n",
       "          [13., 14., 15.],\n",
       "          [16., 17., 18.]]]])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.Tensor([[[[1,2,3],\n",
    "                   [4,5,6],\n",
    "                   [7,8,9]]],\n",
    "                  [[[10,11,12],\n",
    "                   [13,14,15],\n",
    "                   [16,17,18]]]])\n",
    "print(a.shape)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 1, 3, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 1.,  2.,  3.],\n",
       "          [ 4.,  5.,  6.],\n",
       "          [ 7.,  8.,  9.]]],\n",
       "\n",
       "\n",
       "        [[[10., 11., 12.],\n",
       "          [13., 14., 15.],\n",
       "          [16., 17., 18.]]],\n",
       "\n",
       "\n",
       "        [[[ 1.,  2.,  3.],\n",
       "          [ 4.,  5.,  6.],\n",
       "          [ 7.,  8.,  9.]]],\n",
       "\n",
       "\n",
       "        [[[10., 11., 12.],\n",
       "          [13., 14., 15.],\n",
       "          [16., 17., 18.]]],\n",
       "\n",
       "\n",
       "        [[[ 1.,  2.,  3.],\n",
       "          [ 4.,  5.,  6.],\n",
       "          [ 7.,  8.,  9.]]],\n",
       "\n",
       "\n",
       "        [[[10., 11., 12.],\n",
       "          [13., 14., 15.],\n",
       "          [16., 17., 18.]]]])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(a.repeat(3,1,1,1).shape)\n",
    "a.repeat(3,1,1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0.],\n",
       "        [0., 0., 1., 0., 0.],\n",
       "        [0., 0., 1., 0., 0.],\n",
       "        [0., 0., 0., 1., 0.],\n",
       "        [0., 0., 0., 1., 0.],\n",
       "        [0., 0., 0., 1., 0.],\n",
       "        [0., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h = torch.arange(5).reshape(5,1).repeat(1,3).reshape(5*3,1)\n",
    "y_onehot = torch.FloatTensor(5*3, 5)\n",
    "y_onehot.zero_()\n",
    "y_onehot.scatter_(1, h, 1)\n",
    "y_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 1])\n",
      "tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 5\n",
    "nb_digits = 10\n",
    "# Dummy input that HAS to be 2D for the scatter (you can use view(-1,1) if needed)\n",
    "y = torch.LongTensor(batch_size,1).random_() % nb_digits\n",
    "# One hot encoding buffer that you create out of the loop and just keep reusing\n",
    "y_onehot = torch.FloatTensor(batch_size, nb_digits)\n",
    "\n",
    "# In your for loop\n",
    "y_onehot.zero_()\n",
    "y_onehot.scatter_(1, y, 1)\n",
    "\n",
    "print(y.shape)\n",
    "print(y_onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 412.4659, -399.2842]) ==> it's a 0!\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "invalid gradient at index 0 - got [2, 2] but expected shape compatible with [6, 2]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-116-24beaaa9732a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{output.data[0]} ==> it\\'s a {output.data[0].max(0)[1].item()}!'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# loss = torch.nn.functional.cross_entropy(output, )\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meye\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mout0_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py37/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \"\"\"\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py37/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: invalid gradient at index 0 - got [2, 2] but expected shape compatible with [6, 2]"
     ]
    }
   ],
   "source": [
    "x_.requires_grad = True\n",
    "output = nn(x_)\n",
    "print(f'{output.data[0]} ==> it\\'s a {output.data[0].max(0)[1].item()}!')\n",
    "# loss = torch.nn.functional.cross_entropy(output, )\n",
    "output.backward(torch.eye(2))\n",
    "out0_grad = torch.autograd.grad(output[0,:].sum(), x_, retain_graph=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
