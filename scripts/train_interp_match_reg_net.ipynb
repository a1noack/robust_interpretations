{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import setproctitle\n",
    "setproctitle.setproctitle('python4.1')  \n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR, ExponentialLR\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import utils.utils as utils\n",
    "import utils.adv_ex_utils as aus\n",
    "from utils.models import LeNet, DDNet, SimpleCNN\n",
    "from utils.data_loaders import DataLoader\n",
    "from advertorch.attacks import GradientSignAttack, CarliniWagnerL2Attack, PGDAttack\n",
    "\n",
    "# makes default tensor a CUDA tensor so GPU can be used\n",
    "device = torch.device(1 if torch.cuda.is_available() else 'cpu')\n",
    "torch.cuda.set_device(device)\n",
    "if device != 'cpu':\n",
    "    torch.set_default_tensor_type('torch.cuda.FloatTensor')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define train and test functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    net.train()\n",
    "    \n",
    "    for batch_idx, (samples, labels, target_interps) in enumerate(train_loader):\n",
    "        # sends to GPU, i.e. essentially converts from torch.FloatTensor to torch.cuda.FloatTensor\n",
    "        samples, labels, target_interps = samples.to(device), labels.to(device), target_interps.to(device)\n",
    "#         samples = aus.perturb_randomly(samples, scale=.15, min=0., max=1.)\n",
    "#         samples, labels = aus.generate_adv_exs(samples, labels, adversary)\n",
    "                        \n",
    "        optimizer.zero_grad()\n",
    "        output = net(samples)\n",
    "        loss = utils.interp_match_loss(output, labels, x=samples, target_interps=target_interps, \n",
    "                                       net=net, optimizer=optimizer, alpha=alpha)\n",
    "#         loss = utils.my_loss(output, labels, net=net, optimizer=optimizer,\n",
    "#                                         alpha_jr=alpha_jr, x=samples, bp_mat=tr)\n",
    "#         loss = F.cross_entropy(output, labels)\n",
    "#         loss = utils.jacobian_loss(output, labels, x=samples, alpha=alpha_jr, \n",
    "#                                    net=net, bp_mat=tr, optimizer=optimizer)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % log_interval == 0:\n",
    "            j = utils.avg_norm_jacobian(net, samples, output.shape[1], tr, for_loss=False)\n",
    "            i,_ = utils.norm_diff_interp(net, samples, labels, scale=.15, for_loss=False)\n",
    "            print(f'\\tLoss: {loss.item():.6f}, Average norm of Jacobian: {j:6f}, Norm of difference in interpretations: {i:6f}')\n",
    "            train_losses.append(loss.item())\n",
    "            jacobian_norms.append(j)\n",
    "            interp_norm_diffs.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    net.eval()\n",
    "    correct = 0\n",
    "    adv_correct = 0\n",
    "    \n",
    "    for samples, labels, target_interps in test_loader:\n",
    "        samples, labels, target_interps = samples.to(device), labels.to(device), target_interps.to(device)\n",
    "        \n",
    "        output = net(samples)\n",
    "        \n",
    "        if epoch == n_epochs:\n",
    "            adv_samples, adv_labels = aus.generate_adv_exs(samples, labels, adversary, num_per_samp=2)\n",
    "            adv_output = net(adv_samples)\n",
    "            adv_preds = adv_output.data.max(1, keepdim=True)[1]\n",
    "            adv_correct += adv_preds.eq(adv_labels.data.view_as(adv_preds)).sum().item()\n",
    "        \n",
    "        # output is a tensor, .data retrieves its data, max returns the index of the highest valued element\n",
    "        preds = output.data.max(1, keepdim=True)[1]\n",
    "        correct += preds.eq(labels.data.view_as(preds)).sum().item()\n",
    "        \n",
    "    if epoch == n_epochs: \n",
    "        adv_test_accuracy = 100. * float(adv_correct / (len(test_loader.dataset) * 2))\n",
    "        print(f'\\tPGD-perturbed test set accuracy: ({adv_test_accuracy:.2f}%)')\n",
    "        \n",
    "    test_accuracy = 100. * float(correct / len(test_loader.dataset))\n",
    "    \n",
    "    print(f'\\tTest set accuracy: ({test_accuracy:.2f}%)')\n",
    "    \n",
    "    test_accuracies.append(test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #1\n",
      "\tLoss: 4.928511, Average norm of Jacobian: 0.358935, Norm of difference in interpretations: 0.003409\n",
      "\tLoss: 4.933461, Average norm of Jacobian: 2.488898, Norm of difference in interpretations: 0.014809\n",
      "\tLoss: 4.623224, Average norm of Jacobian: 27.271135, Norm of difference in interpretations: 0.175872\n",
      "\tTest set accuracy: (29.08%)\n",
      "Epoch #2\n",
      "\tLoss: 4.573931, Average norm of Jacobian: 38.527721, Norm of difference in interpretations: 0.222470\n",
      "\tLoss: 4.153757, Average norm of Jacobian: 58.362129, Norm of difference in interpretations: 0.720090\n",
      "\tLoss: 4.106948, Average norm of Jacobian: 111.437485, Norm of difference in interpretations: 3.101839\n",
      "\tTest set accuracy: (34.26%)\n",
      "Epoch #3\n",
      "\tLoss: 4.195321, Average norm of Jacobian: 152.696884, Norm of difference in interpretations: 1.755640\n",
      "\tLoss: 4.241650, Average norm of Jacobian: 242.431992, Norm of difference in interpretations: 3.294142\n",
      "\tLoss: 4.113476, Average norm of Jacobian: 314.152100, Norm of difference in interpretations: 1.509639\n",
      "\tTest set accuracy: (35.29%)\n",
      "Epoch #4\n",
      "\tLoss: 3.960301, Average norm of Jacobian: 311.436676, Norm of difference in interpretations: 3.845836\n",
      "\tLoss: 3.798146, Average norm of Jacobian: 377.888428, Norm of difference in interpretations: 4.320160\n",
      "\tLoss: 3.850131, Average norm of Jacobian: 393.398468, Norm of difference in interpretations: 9.488994\n",
      "\tTest set accuracy: (39.82%)\n",
      "Epoch #5\n",
      "\tLoss: 4.737558, Average norm of Jacobian: 323.200806, Norm of difference in interpretations: 6.712607\n",
      "\tLoss: 3.777002, Average norm of Jacobian: 359.050079, Norm of difference in interpretations: 7.387911\n",
      "\tLoss: 3.671582, Average norm of Jacobian: 352.194580, Norm of difference in interpretations: 5.996769\n",
      "\tTest set accuracy: (45.55%)\n",
      "Epoch #6\n",
      "\tLoss: 3.890039, Average norm of Jacobian: 428.689972, Norm of difference in interpretations: 4.179820\n",
      "\tLoss: 3.619866, Average norm of Jacobian: 393.469727, Norm of difference in interpretations: 4.383479\n",
      "\tLoss: 3.458018, Average norm of Jacobian: 422.021698, Norm of difference in interpretations: 7.646269\n",
      "\tTest set accuracy: (46.88%)\n",
      "Epoch #7\n",
      "\tLoss: 3.381780, Average norm of Jacobian: 440.675323, Norm of difference in interpretations: 3.646253\n",
      "\tLoss: 3.598060, Average norm of Jacobian: 477.402008, Norm of difference in interpretations: 5.904062\n",
      "\tLoss: 3.249094, Average norm of Jacobian: 442.837402, Norm of difference in interpretations: 5.133502\n",
      "\tTest set accuracy: (52.24%)\n",
      "Epoch #8\n",
      "\tLoss: 3.057528, Average norm of Jacobian: 416.087280, Norm of difference in interpretations: 4.226827\n",
      "\tLoss: 2.986469, Average norm of Jacobian: 511.302643, Norm of difference in interpretations: 5.575095\n",
      "\tLoss: 2.989513, Average norm of Jacobian: 511.675354, Norm of difference in interpretations: 7.771134\n",
      "\tTest set accuracy: (54.57%)\n",
      "Epoch #9\n",
      "\tLoss: 2.830642, Average norm of Jacobian: 471.632874, Norm of difference in interpretations: 5.196220\n",
      "\tLoss: 2.881967, Average norm of Jacobian: 531.090332, Norm of difference in interpretations: 4.354487\n",
      "\tLoss: 2.682654, Average norm of Jacobian: 457.522583, Norm of difference in interpretations: 5.031121\n",
      "\tTest set accuracy: (57.98%)\n",
      "Epoch #10\n",
      "\tLoss: 2.714139, Average norm of Jacobian: 490.485199, Norm of difference in interpretations: 5.176928\n",
      "\tLoss: 3.013718, Average norm of Jacobian: 501.057007, Norm of difference in interpretations: 5.648458\n",
      "\tLoss: 2.991014, Average norm of Jacobian: 496.302765, Norm of difference in interpretations: 5.250217\n",
      "\tTest set accuracy: (60.50%)\n",
      "Epoch #11\n",
      "\tLoss: 2.908898, Average norm of Jacobian: 503.076630, Norm of difference in interpretations: 8.075774\n",
      "\tLoss: 2.772657, Average norm of Jacobian: 491.370300, Norm of difference in interpretations: 4.649152\n",
      "\tLoss: 2.452801, Average norm of Jacobian: 490.227997, Norm of difference in interpretations: 5.937536\n",
      "\tTest set accuracy: (64.01%)\n",
      "Epoch #12\n",
      "\tLoss: 2.548407, Average norm of Jacobian: 497.959473, Norm of difference in interpretations: 6.158603\n",
      "\tLoss: 2.504080, Average norm of Jacobian: 509.408173, Norm of difference in interpretations: 4.998451\n",
      "\tLoss: 2.821472, Average norm of Jacobian: 520.608826, Norm of difference in interpretations: 5.669051\n",
      "\tTest set accuracy: (65.02%)\n",
      "Epoch #13\n",
      "\tLoss: 2.998153, Average norm of Jacobian: 520.005676, Norm of difference in interpretations: 4.853290\n",
      "\tLoss: 2.747556, Average norm of Jacobian: 484.581146, Norm of difference in interpretations: 8.170529\n",
      "\tLoss: 2.462400, Average norm of Jacobian: 498.637573, Norm of difference in interpretations: 4.303279\n",
      "\tTest set accuracy: (65.33%)\n",
      "Epoch #14\n",
      "\tLoss: 2.868054, Average norm of Jacobian: 488.470337, Norm of difference in interpretations: 4.894198\n",
      "\tLoss: 3.250359, Average norm of Jacobian: 509.389862, Norm of difference in interpretations: 5.863780\n"
     ]
    }
   ],
   "source": [
    "dataset = 'CIFAR-10'\n",
    "\n",
    "# training details\n",
    "n_epochs = 30\n",
    "log_interval = 200\n",
    "# torch.manual_seed(7)\n",
    "# alphas = [0., 1e-3, 1e-2, 7e-2]\n",
    "# alphas = [.75e-2, .75e-2, .75e-2, .75e-2]\n",
    "alphas = [1e-1]*5\n",
    "alphas = [2.5e-2]\n",
    "# alpha = 7e-2\n",
    "# alpha_jrs = [.015, .02, .025]\n",
    "# alpha_jrs = [1.1e-3, 1.1e-3, 1.1e-3, 1.1e-3]\n",
    "alpha_jrs = [2.8e-3, 2.8e-3, 2.8e-3, 2.8e-3]\n",
    "# threshs = [None, -2., -1., 0., 1., 2., 2.5]\n",
    "thresh=0\n",
    "\n",
    "# dictionary to record each model's training/testing stats\n",
    "performance = {}\n",
    "q = 0\n",
    "\n",
    "for alpha in alphas:\n",
    "    dl = DataLoader(dataset='CIFAR-10_interps_stdtrain', tr_batch_size=100, te_batch_size=100, \n",
    "                    augment=False, path='../data', thresh=thresh, model='lenet')\n",
    "#     dl = DataLoader(dataset='MNIST', tr_batch_size=64, te_batch_size=100, \n",
    "#                     augment=False, path='../data', thresh=thresh)\n",
    "    train_loader = dl.train_loader\n",
    "    test_loader = dl.test_loader\n",
    "    tr_batch_size = dl.tr_batch_size\n",
    "    te_batch_size = dl.te_batch_size\n",
    "    # create matrices for back propagation\n",
    "    tr = utils.bp_matrix(tr_batch_size, 10)\n",
    "    te = utils.bp_matrix(te_batch_size, 10)\n",
    "    # instantiate model and optimizer\n",
    "    learning_rate = 0.01\n",
    "    momentum = 0.9\n",
    "    net = DDNet(p_drop=.5)\n",
    "#     net = SimpleCNN(p_drop=.5)\n",
    "    optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=momentum)#, weight_decay=.0005)\n",
    "    lr_decayer = StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "#     lr_decayer = ExponentialLR(optimizer, gamma=.95)\n",
    "\n",
    "    # make model CUDA enabled and define GPU/device to use\n",
    "    net.cuda()\n",
    "\n",
    "    # define adversary to train against if needed\n",
    "    adversary = PGDAttack(predict=net, loss_fn=F.cross_entropy, eps=.314, \n",
    "                          nb_iter=40, eps_iter=.04, rand_init=True, \n",
    "                          clip_min=0., clip_max=1., ord=2, targeted=False)\n",
    "\n",
    "    # for tracking training progress\n",
    "    train_losses = []\n",
    "    test_accuracies = []\n",
    "    jacobian_norms = []\n",
    "    interp_norm_diffs = []\n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        print(f'Epoch #{epoch}')\n",
    "        train()\n",
    "        test()\n",
    "        lr_decayer.step()\n",
    "\n",
    "    #     performance['simplecnn_alpha{}'.format(alpha)] = (train_losses, test_accuracies, \n",
    "    #                                               jacobian_norms, interp_norm_diffs)\n",
    "#     torch.save(net.state_dict(), \n",
    "#                f'../trained_models/{dataset}/simplecnn_jr/model{q}')\n",
    "    q+=1\n",
    "\n",
    "    ### Write performance dictionary to text file\n",
    "\n",
    "#     f = open(f'../trained_models/{dataset}/simplecnn_stdtrain/performance.txt','w')\n",
    "#     f.write(str(performance))\n",
    "#     f.close()\n",
    "\n",
    "    # to read dictionary from file:\n",
    "    # f = open(f'models/training_round_{training_round}_performance.txt','r')\n",
    "    # d = eval(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), \n",
    "               f'../trained_models/{dataset}/ddnet_stdtrain/model{0}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
