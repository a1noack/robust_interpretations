{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import utils.utils as utils\n",
    "import utils.adv_ex_utils as aus\n",
    "from utils.models import LeNet, DDNet, SimpleCNN\n",
    "from utils.data_loaders import DataLoader\n",
    "from advertorch.attacks import GradientSignAttack, CarliniWagnerL2Attack, PGDAttack\n",
    "\n",
    "# makes default tensor a CUDA tensor so GPU can be used\n",
    "device = torch.device(2 if torch.cuda.is_available() else 'cpu')\n",
    "torch.cuda.set_device(device)\n",
    "if device != 'cpu':\n",
    "    torch.set_default_tensor_type('torch.cuda.FloatTensor')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define train and test functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    net.train()\n",
    "    \n",
    "    for batch_idx, (samples,labels,target_interps) in enumerate(train_loader):\n",
    "        # sends to GPU, i.e. essentially converts from torch.FloatTensor to torch.cuda.FloatTensor\n",
    "        samples,labels,target_interps = samples.to(device),labels.to(device),target_interps.to(device)\n",
    "        samples = aus.perturb_randomly(samples, scale=.15, min=0., max=1.)\n",
    "                        \n",
    "        optimizer.zero_grad()\n",
    "        output = net(samples)\n",
    "        loss = utils.interp_match_loss(output, labels, x=samples, target_interps=target_interps, \n",
    "                                       net=net, optimizer=optimizer, alpha=alpha)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % log_interval == 0:\n",
    "            j = utils.avg_norm_jacobian(net, samples, output.shape[1], tr, for_loss=False)\n",
    "            i,_ = utils.norm_diff_interp(net, samples, labels, scale=.15, for_loss=False)\n",
    "            print(f'\\tLoss: {loss.item():.6f}, Average norm of Jacobian: {j:6f}, Norm of difference in interpretations: {i:6f}')\n",
    "            train_losses.append(loss.item())\n",
    "            jacobian_norms.append(j)\n",
    "            interp_norm_diffs.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    net.eval()\n",
    "    correct = 0\n",
    "    adv_correct = 0\n",
    "    \n",
    "    for samples,labels,target_interps in test_loader:\n",
    "        samples,labels,target_interps = samples.to(device),labels.to(device),target_interps.to(device)\n",
    "        \n",
    "        output = net(samples)\n",
    "        \n",
    "        adv_samples,labels = aus.generate_adv_exs(samples, labels, adversary)\n",
    "        adv_output = net(adv_samples)\n",
    "        \n",
    "        # output is a tensor, .data retrieves its data, max returns the index of the highest valued element\n",
    "        preds = output.data.max(1, keepdim=True)[1]\n",
    "        correct += preds.eq(labels.data.view_as(preds)).sum().item()\n",
    "        \n",
    "        adv_preds = adv_output.data.max(1, keepdim=True)[1]\n",
    "        adv_correct += adv_preds.eq(labels.data.view_as(adv_preds)).sum().item()\n",
    "                \n",
    "    test_accuracy = 100. * float(correct / len(test_loader.dataset))\n",
    "    adv_test_accuracy = 100. * float(adv_correct / len(test_loader.dataset))\n",
    "    \n",
    "    print(f'\\tTtest set accuracy: ({test_accuracy:.2f}%)')\n",
    "    print(f'\\tPGD-perturbed test set accuracy: ({adv_test_accuracy:.2f}%)')\n",
    "    \n",
    "    test_accuracies.append(test_accuracy)\n",
    "    adv_test_accuracies.append(adv_test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #1\n",
      "\tLoss: 2.302015, Average norm of Jacobian: 0.002556, Norm of difference in interpretations: 1.145142\n",
      "\tLoss: 1.794961, Average norm of Jacobian: 0.332941, Norm of difference in interpretations: 6.133467\n",
      "\tLoss: 1.738459, Average norm of Jacobian: 0.443577, Norm of difference in interpretations: 9.629074\n",
      "\tLoss: 1.566728, Average norm of Jacobian: 0.287748, Norm of difference in interpretations: 11.642480\n",
      "\tLoss: 1.602690, Average norm of Jacobian: 0.263026, Norm of difference in interpretations: 13.430804\n",
      "\tTtest set accuracy: (85.96%)\n",
      "\tPGD-perturbed test set accuracy: (58.13%)\n",
      "Epoch #2\n",
      "\tLoss: 1.650705, Average norm of Jacobian: 0.488221, Norm of difference in interpretations: 13.006514\n",
      "\tLoss: 1.580172, Average norm of Jacobian: 0.261305, Norm of difference in interpretations: 16.123606\n",
      "\tLoss: 1.522096, Average norm of Jacobian: 0.244330, Norm of difference in interpretations: 18.252075\n",
      "\tLoss: 1.536858, Average norm of Jacobian: 0.352778, Norm of difference in interpretations: 20.227837\n",
      "\tLoss: 1.499931, Average norm of Jacobian: 0.171702, Norm of difference in interpretations: 21.163187\n",
      "\tTtest set accuracy: (96.63%)\n",
      "\tPGD-perturbed test set accuracy: (71.06%)\n",
      "Epoch #3\n",
      "\tLoss: 1.522856, Average norm of Jacobian: 0.188619, Norm of difference in interpretations: 23.112375\n",
      "\tLoss: 1.521861, Average norm of Jacobian: 0.291705, Norm of difference in interpretations: 24.186207\n",
      "\tLoss: 1.485014, Average norm of Jacobian: 0.359453, Norm of difference in interpretations: 24.984251\n",
      "\tLoss: 1.525232, Average norm of Jacobian: 0.482490, Norm of difference in interpretations: 25.766790\n",
      "\tLoss: 1.531437, Average norm of Jacobian: 0.137473, Norm of difference in interpretations: 25.404882\n",
      "\tTtest set accuracy: (97.37%)\n",
      "\tPGD-perturbed test set accuracy: (70.41%)\n",
      "Epoch #4\n",
      "\tLoss: 1.528895, Average norm of Jacobian: 0.490215, Norm of difference in interpretations: 26.727671\n",
      "\tLoss: 1.531490, Average norm of Jacobian: 0.296630, Norm of difference in interpretations: 28.139624\n",
      "\tLoss: 1.480445, Average norm of Jacobian: 0.277378, Norm of difference in interpretations: 30.156279\n",
      "\tLoss: 1.504167, Average norm of Jacobian: 0.473895, Norm of difference in interpretations: 29.396971\n",
      "\tLoss: 1.467428, Average norm of Jacobian: 0.302843, Norm of difference in interpretations: 32.165504\n",
      "\tTtest set accuracy: (97.52%)\n",
      "\tPGD-perturbed test set accuracy: (73.02%)\n",
      "Epoch #5\n",
      "\tLoss: 1.491490, Average norm of Jacobian: 0.328648, Norm of difference in interpretations: 30.939190\n",
      "\tLoss: 1.484143, Average norm of Jacobian: 0.534543, Norm of difference in interpretations: 33.412224\n",
      "\tLoss: 1.510598, Average norm of Jacobian: 0.298084, Norm of difference in interpretations: 31.240484\n",
      "\tLoss: 1.533547, Average norm of Jacobian: 0.093833, Norm of difference in interpretations: 31.894903\n",
      "\tLoss: 1.476739, Average norm of Jacobian: 0.220798, Norm of difference in interpretations: 33.383636\n",
      "\tTtest set accuracy: (97.92%)\n",
      "\tPGD-perturbed test set accuracy: (74.42%)\n",
      "Epoch #6\n",
      "\tLoss: 1.493884, Average norm of Jacobian: 0.240063, Norm of difference in interpretations: 33.847961\n",
      "\tLoss: 1.490965, Average norm of Jacobian: 0.840485, Norm of difference in interpretations: 33.388607\n",
      "\tLoss: 1.539722, Average norm of Jacobian: 0.561818, Norm of difference in interpretations: 36.112469\n",
      "\tLoss: 1.515852, Average norm of Jacobian: 0.960106, Norm of difference in interpretations: 36.691559\n",
      "\tLoss: 1.484759, Average norm of Jacobian: 0.101917, Norm of difference in interpretations: 35.725117\n"
     ]
    }
   ],
   "source": [
    "dataset = 'MNIST'\n",
    "\n",
    "dl = DataLoader(dataset='MNIST_interps', augment=False, model='lenet', path='../data', thresh=1.)\n",
    "train_loader = dl.train_loader\n",
    "test_loader = dl.test_loader\n",
    "tr_batch_size = dl.tr_batch_size\n",
    "te_batch_size = dl.te_batch_size\n",
    "\n",
    "# training details\n",
    "n_epochs = 30\n",
    "log_interval = 200\n",
    "training_round = 3\n",
    "torch.manual_seed(7)\n",
    "alphas = [0, 1e-3, 4e-3, 7e-3, 1e-2, 4e-2, 7e-2, 1e-1]\n",
    "\n",
    "# dictionary to record each model's training/testing stats\n",
    "performance = {}\n",
    "\n",
    "# create matrices for back propagation\n",
    "tr = utils.bp_matrix(tr_batch_size, 10)\n",
    "te = utils.bp_matrix(te_batch_size, 10)\n",
    "\n",
    "for alpha in alphas:\n",
    "    # instantiate model and optimizer\n",
    "    learning_rate = 0.01\n",
    "    momentum = 0.9\n",
    "    net = SimpleCNN()\n",
    "    optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=momentum)\n",
    "    lr_decayer = StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "    # make model CUDA enabled and define GPU/device to use\n",
    "    net.cuda()\n",
    "\n",
    "    # define adversary to train against if needed\n",
    "    adversary = PGDAttack(predict=net, loss_fn=F.cross_entropy, eps=1.5, \n",
    "                          nb_iter=40, eps_iter=0.05, rand_init=True, \n",
    "                          clip_min=0., clip_max=1., ord=2, targeted=False)\n",
    "\n",
    "    # for tracking training progress\n",
    "    train_losses = []\n",
    "    test_accuracies = []\n",
    "    adv_test_accuracies = []\n",
    "    jacobian_norms = []\n",
    "    interp_norm_diffs = []\n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        print(f'Epoch #{epoch}')\n",
    "        train()\n",
    "        test()\n",
    "        lr_decayer.step()\n",
    "\n",
    "    performance[f'simplecnn_alpha{alpha}'] = (train_losses, test_accuracies, adv_test_accuracies, jacobian_norms, interp_norm_diffs)\n",
    "    torch.save(net.state_dict(), f'../trained_models/{dataset}/interp_match_reg2/simplecnn_alpha{alpha}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write performance dictionary to text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(f'../trained_models/{dataset}/interp_match_reg2/performance.txt','w')\n",
    "f.write(str(performance))\n",
    "f.close()\n",
    "\n",
    "# to read dictionary from file:\n",
    "# f = open(f'models/training_round_{training_round}_performance.txt','r')\n",
    "# d = eval(f.read())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
