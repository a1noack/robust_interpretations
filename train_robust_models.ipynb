{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Code assumes the ability to train using a GPU with CUDA.\n",
    "\"\"\"\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from advertorch.attacks import GradientSignAttack, CarliniWagnerL2Attack, PGDAttack\n",
    "import matplotlib.pyplot as plt\n",
    "import utils.adv_ex_utils as aus\n",
    "import utils.interp_generators as igs\n",
    "import utils.utils as utils\n",
    "from utils.models import LeNet, DDNet\n",
    "from utils.data_loaders import DataLoader\n",
    "\n",
    "# makes default tensor a CUDA tensor so GPU can be used\n",
    "device = torch.device(1 if torch.cuda.is_available() else 'cpu')\n",
    "torch.cuda.set_device(device)\n",
    "if device != 'cpu':\n",
    "    torch.set_default_tensor_type('torch.cuda.FloatTensor')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define data loaders and data preprocessing steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "dataset = 'CIFAR-10'\n",
    "\n",
    "dl = DataLoader(dataset=dataset, augment=False)\n",
    "train_loader = dl.train_loader\n",
    "test_loader = dl.test_loader\n",
    "tr_batch_size = dl.tr_batch_size\n",
    "te_batch_size = dl.te_batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define train and test functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    net.train()\n",
    "    \n",
    "    for batch_idx, (samples, labels) in enumerate(train_loader):\n",
    "        # sends to GPU, i.e. essentially converts from torch.FloatTensor to torch.cuda.FloatTensor\n",
    "        samples, labels = samples.to(device), labels.to(device)\n",
    "        \n",
    "        # expand dataset with adversarial examples if adversary specified\n",
    "        if adversary != None:\n",
    "            adv_samples, adv_labels = aus.generate_adv_exs(samples, labels, adversary)\n",
    "            samples, labels = torch.cat([samples, adv_samples], 0), torch.cat([labels, adv_labels], 0)\n",
    "                \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = net(samples)\n",
    "        \n",
    "        loss = utils.my_loss(output, labels, net, optimizer,\n",
    "                             alpha_wd=alpha_wd, alpha_jr=alpha_jr, \n",
    "                             x=samples, bp_mat=tr)\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % log_interval == 0:\n",
    "            j = utils.avg_norm_jacobian(net, samples, output.shape[1], tr, for_loss=False)\n",
    "            print(f'\\tLoss: {loss.item():.6f} Average norm of Jacobian: {j:6f}')\n",
    "            train_losses.append(loss.item())\n",
    "            jacobian_norms.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    for samples, labels in test_loader:\n",
    "        samples, labels = samples.to(device), labels.to(device)\n",
    "        output = net(samples)\n",
    "        test_loss += utils.my_loss(output, labels, net, optimizer,\n",
    "                                   alpha_wd=alpha_wd, alpha_jr=alpha_jr, \n",
    "                                   x=samples, bp_mat=te).item()\n",
    "        # output is a tensor, .data retrieves its data, max returns the index of the highest valued element\n",
    "        preds = output.data.max(1, keepdim=True)[1]\n",
    "        correct += preds.eq(labels.data.view_as(preds)).sum().item()\n",
    "                \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_accuracy = 100. * float(correct / len(test_loader.dataset))\n",
    "    \n",
    "    print(f'\\tTest set accuracy: ({test_accuracy:.2f}%)')\n",
    "    \n",
    "    test_accuracies.append(test_accuracy)\n",
    "    test_losses.append(test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Beginning training for model: trained_models/CIFAR-10/training_round_1/ep0_wd0_jr0.01_1\n",
      "Epoch #1\n",
      "\tLoss: 2.311766 Average norm of Jacobian: 0.001591\n",
      "\tLoss: 1.953117 Average norm of Jacobian: 0.141008\n",
      "\tLoss: 1.500202 Average norm of Jacobian: 1.335765\n",
      "\tLoss: 1.524923 Average norm of Jacobian: 2.332409\n",
      "\tTest set accuracy: (52.93%)\n",
      "Epoch #2\n",
      "\tLoss: 1.543494 Average norm of Jacobian: 2.835848\n",
      "\tLoss: 1.166420 Average norm of Jacobian: 6.567211\n",
      "\tLoss: 1.173705 Average norm of Jacobian: 7.201236\n",
      "\tLoss: 1.287471 Average norm of Jacobian: 9.942127\n",
      "\tTest set accuracy: (63.21%)\n",
      "Epoch #3\n",
      "\tLoss: 1.238295 Average norm of Jacobian: 7.961130\n",
      "\tLoss: 0.924732 Average norm of Jacobian: 14.440787\n",
      "\tLoss: 0.948359 Average norm of Jacobian: 9.668236\n",
      "\tLoss: 0.839496 Average norm of Jacobian: 13.218650\n",
      "\tTest set accuracy: (68.96%)\n",
      "Epoch #4\n",
      "\tLoss: 1.048609 Average norm of Jacobian: 11.521887\n",
      "\tLoss: 0.987433 Average norm of Jacobian: 11.495178\n",
      "\tLoss: 1.091940 Average norm of Jacobian: 14.036659\n",
      "\tLoss: 0.694132 Average norm of Jacobian: 15.281530\n",
      "\tTest set accuracy: (73.46%)\n",
      "Epoch #5\n",
      "\tLoss: 0.957108 Average norm of Jacobian: 12.798601\n",
      "\tLoss: 0.888796 Average norm of Jacobian: 12.672327\n",
      "\tLoss: 0.881277 Average norm of Jacobian: 15.062163\n",
      "\tLoss: 1.066781 Average norm of Jacobian: 15.951050\n",
      "\tTest set accuracy: (72.97%)\n",
      "Epoch #6\n",
      "\tLoss: 0.794701 Average norm of Jacobian: 15.771726\n",
      "\tLoss: 0.866286 Average norm of Jacobian: 14.546809\n",
      "\tLoss: 0.911445 Average norm of Jacobian: 14.455743\n",
      "\tLoss: 1.108062 Average norm of Jacobian: 18.375893\n",
      "\tTest set accuracy: (72.16%)\n",
      "Epoch #7\n",
      "\tLoss: 0.975345 Average norm of Jacobian: 17.040325\n",
      "\tLoss: 0.896813 Average norm of Jacobian: 15.816197\n",
      "\tLoss: 0.708212 Average norm of Jacobian: 15.271989\n",
      "\tLoss: 0.720483 Average norm of Jacobian: 14.722378\n",
      "\tTest set accuracy: (74.46%)\n",
      "Epoch #8\n",
      "\tLoss: 0.688435 Average norm of Jacobian: 16.889336\n",
      "\tLoss: 0.878903 Average norm of Jacobian: 18.676991\n",
      "\tLoss: 0.749533 Average norm of Jacobian: 16.700397\n",
      "\tLoss: 0.913984 Average norm of Jacobian: 16.989334\n",
      "\tTest set accuracy: (74.22%)\n",
      "Epoch #9\n",
      "\tLoss: 0.724410 Average norm of Jacobian: 15.308035\n",
      "\tLoss: 0.581943 Average norm of Jacobian: 17.693157\n",
      "\tLoss: 0.864352 Average norm of Jacobian: 18.988169\n",
      "\tLoss: 0.851194 Average norm of Jacobian: 16.797607\n",
      "\tTest set accuracy: (73.10%)\n",
      "Epoch #10\n",
      "\tLoss: 0.861833 Average norm of Jacobian: 19.736996\n",
      "\tLoss: 0.753133 Average norm of Jacobian: 23.124050\n",
      "\tLoss: 0.746493 Average norm of Jacobian: 16.779669\n",
      "\tLoss: 0.642627 Average norm of Jacobian: 19.168770\n",
      "\tTest set accuracy: (74.36%)\n",
      "Epoch #11\n",
      "\tLoss: 0.875142 Average norm of Jacobian: 18.394859\n",
      "\tLoss: 0.499345 Average norm of Jacobian: 21.921970\n",
      "\tLoss: 0.559718 Average norm of Jacobian: 23.701250\n",
      "\tLoss: 0.460477 Average norm of Jacobian: 20.915619\n",
      "\tTest set accuracy: (79.44%)\n",
      "Epoch #12\n",
      "\tLoss: 0.612541 Average norm of Jacobian: 21.373825\n",
      "\tLoss: 0.620838 Average norm of Jacobian: 21.427132\n",
      "\tLoss: 0.550455 Average norm of Jacobian: 22.570589\n",
      "\tLoss: 0.576020 Average norm of Jacobian: 23.714540\n",
      "\tTest set accuracy: (79.47%)\n",
      "Epoch #13\n",
      "\tLoss: 0.643110 Average norm of Jacobian: 21.822073\n",
      "\tLoss: 0.625252 Average norm of Jacobian: 20.073473\n",
      "\tLoss: 0.633864 Average norm of Jacobian: 21.657684\n",
      "\tLoss: 0.641230 Average norm of Jacobian: 22.482315\n",
      "\tTest set accuracy: (79.25%)\n",
      "Epoch #14\n",
      "\tLoss: 0.605786 Average norm of Jacobian: 21.171833\n",
      "\tLoss: 0.530500 Average norm of Jacobian: 21.113972\n",
      "\tLoss: 0.741287 Average norm of Jacobian: 21.504871\n",
      "\tLoss: 0.711490 Average norm of Jacobian: 20.291313\n",
      "\tTest set accuracy: (78.23%)\n",
      "Epoch #15\n",
      "\tLoss: 0.526610 Average norm of Jacobian: 21.347328\n",
      "\tLoss: 0.489496 Average norm of Jacobian: 20.649311\n",
      "\tLoss: 0.600024 Average norm of Jacobian: 21.375076\n",
      "\tLoss: 0.563294 Average norm of Jacobian: 22.237940\n",
      "\tTest set accuracy: (79.41%)\n",
      "Epoch #16\n",
      "\tLoss: 0.487575 Average norm of Jacobian: 21.989168\n",
      "\tLoss: 0.469690 Average norm of Jacobian: 22.008881\n",
      "\tLoss: 0.560122 Average norm of Jacobian: 21.084051\n",
      "\tLoss: 0.436785 Average norm of Jacobian: 21.944103\n",
      "\tTest set accuracy: (79.42%)\n",
      "Epoch #17\n",
      "\tLoss: 0.557130 Average norm of Jacobian: 23.109694\n",
      "\tLoss: 0.644166 Average norm of Jacobian: 20.638691\n",
      "\tLoss: 0.499365 Average norm of Jacobian: 20.169727\n",
      "\tLoss: 0.640369 Average norm of Jacobian: 23.084396\n",
      "\tTest set accuracy: (79.52%)\n",
      "Epoch #18\n",
      "\tLoss: 0.461382 Average norm of Jacobian: 21.410315\n",
      "\tLoss: 0.557676 Average norm of Jacobian: 20.374981\n",
      "\tLoss: 0.559344 Average norm of Jacobian: 21.032356\n",
      "\tLoss: 0.567498 Average norm of Jacobian: 21.112389\n",
      "\tTest set accuracy: (79.39%)\n",
      "Epoch #19\n",
      "\tLoss: 0.648069 Average norm of Jacobian: 19.974104\n",
      "\tLoss: 0.568240 Average norm of Jacobian: 21.010881\n",
      "\tLoss: 0.610533 Average norm of Jacobian: 21.789988\n",
      "\tLoss: 0.557304 Average norm of Jacobian: 21.062056\n",
      "\tTest set accuracy: (79.61%)\n",
      "Epoch #20\n",
      "\tLoss: 0.493724 Average norm of Jacobian: 22.374695\n",
      "\tLoss: 0.608669 Average norm of Jacobian: 20.240257\n",
      "\tLoss: 0.407728 Average norm of Jacobian: 21.925501\n",
      "\tLoss: 0.490554 Average norm of Jacobian: 21.896564\n",
      "\tTest set accuracy: (79.50%)\n",
      "Epoch #21\n",
      "\tLoss: 0.449149 Average norm of Jacobian: 21.369568\n",
      "\tLoss: 0.586797 Average norm of Jacobian: 22.307831\n",
      "\tLoss: 0.591722 Average norm of Jacobian: 21.673811\n",
      "\tLoss: 0.356543 Average norm of Jacobian: 22.661613\n",
      "\tTest set accuracy: (79.88%)\n",
      "Epoch #22\n",
      "\tLoss: 0.460188 Average norm of Jacobian: 22.095358\n",
      "\tLoss: 0.623317 Average norm of Jacobian: 22.010891\n",
      "\tLoss: 0.566505 Average norm of Jacobian: 21.444466\n",
      "\tLoss: 0.504767 Average norm of Jacobian: 22.198433\n",
      "\tTest set accuracy: (79.94%)\n",
      "Epoch #23\n",
      "\tLoss: 0.407929 Average norm of Jacobian: 23.441706\n",
      "\tLoss: 0.572765 Average norm of Jacobian: 20.678812\n",
      "\tLoss: 0.412535 Average norm of Jacobian: 23.266998\n",
      "\tLoss: 0.455283 Average norm of Jacobian: 22.700567\n",
      "\tTest set accuracy: (79.70%)\n",
      "Epoch #24\n",
      "\tLoss: 0.507612 Average norm of Jacobian: 22.736904\n",
      "\tLoss: 0.437205 Average norm of Jacobian: 21.884983\n",
      "\tLoss: 0.438030 Average norm of Jacobian: 21.960659\n",
      "\tLoss: 0.390024 Average norm of Jacobian: 22.444998\n",
      "\tTest set accuracy: (79.94%)\n",
      "Epoch #25\n",
      "\tLoss: 0.586264 Average norm of Jacobian: 22.286360\n",
      "\tLoss: 0.414533 Average norm of Jacobian: 21.976530\n",
      "\tLoss: 0.569829 Average norm of Jacobian: 22.902035\n",
      "\tLoss: 0.642312 Average norm of Jacobian: 21.340805\n",
      "\tTest set accuracy: (79.93%)\n",
      "Epoch #26\n",
      "\tLoss: 0.505971 Average norm of Jacobian: 22.676311\n",
      "\tLoss: 0.455310 Average norm of Jacobian: 21.890469\n",
      "\tLoss: 0.518030 Average norm of Jacobian: 23.077427\n",
      "\tLoss: 0.494474 Average norm of Jacobian: 22.031681\n",
      "\tTest set accuracy: (80.03%)\n",
      "Epoch #27\n",
      "\tLoss: 0.511831 Average norm of Jacobian: 22.203701\n",
      "\tLoss: 0.525780 Average norm of Jacobian: 23.048531\n",
      "\tLoss: 0.504157 Average norm of Jacobian: 21.869835\n",
      "\tLoss: 0.536807 Average norm of Jacobian: 22.338892\n",
      "\tTest set accuracy: (79.96%)\n",
      "Epoch #28\n",
      "\tLoss: 0.354175 Average norm of Jacobian: 22.541420\n",
      "\tLoss: 0.509804 Average norm of Jacobian: 21.858210\n",
      "\tLoss: 0.504273 Average norm of Jacobian: 22.819941\n",
      "\tLoss: 0.442962 Average norm of Jacobian: 22.502583\n",
      "\tTest set accuracy: (79.59%)\n",
      "Epoch #29\n",
      "\tLoss: 0.395261 Average norm of Jacobian: 21.993727\n",
      "\tLoss: 0.456044 Average norm of Jacobian: 22.868908\n",
      "\tLoss: 0.471598 Average norm of Jacobian: 22.967146\n",
      "\tLoss: 0.472516 Average norm of Jacobian: 22.070494\n",
      "\tTest set accuracy: (79.99%)\n",
      "Epoch #30\n",
      "\tLoss: 0.555725 Average norm of Jacobian: 22.865877\n",
      "\tLoss: 0.486410 Average norm of Jacobian: 22.279968\n",
      "\tLoss: 0.470125 Average norm of Jacobian: 22.940853\n",
      "\tLoss: 0.506010 Average norm of Jacobian: 23.077026\n",
      "\tTest set accuracy: (79.98%)\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'trained_models/CIFAR-10/training_round_1/ep0_wd0_jr0.01_1'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-350767f21cb9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0mperformance\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mf'ep{epsilon}_wd{alpha_wd}_jr{alpha_jr}_{training_round}'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_accuracies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjacobian_norms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'trained_models/{dataset}/training_round_{training_round}/ep{epsilon}_wd{alpha_wd}_jr{alpha_jr}_{training_round}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py37/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;34m>>\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \"\"\"\n\u001b[0;32m--> 224\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_with_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py37/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_with_file_like\u001b[0;34m(f, mode, body)\u001b[0m\n\u001b[1;32m    145\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpathlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0mnew_fd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'trained_models/CIFAR-10/training_round_1/ep0_wd0_jr0.01_1'"
     ]
    }
   ],
   "source": [
    "# training details\n",
    "n_epochs = 30\n",
    "log_interval = 200\n",
    "training_round = 1\n",
    "torch.manual_seed(training_round)\n",
    "\n",
    "# varying values for certain hyperparameters to produce models with varying degrees of robustness\n",
    "epsilons = [0, .1, .2, .3, .4, .5, .6]\n",
    "alpha_wds = [0, .0000001, .000001, .00001, .0001, .001, .01]\n",
    "alpha_jrs = [0.01, .0000001, .000001, .00001, .0001, .001, .01]\n",
    "\n",
    "# dictionary to record each model's training/testing stats\n",
    "performance = {}\n",
    "\n",
    "for hyp_param_to_vary in [alpha_jrs, epsilons, alpha_wds]:\n",
    "    epsilon = 0\n",
    "    alpha_wd = 0\n",
    "    alpha_jr = 0\n",
    "    \n",
    "    model_name = f'ep{epsilon}_wd{alpha_wd}_jr{alpha_jr}_{training_round}'\n",
    "    \n",
    "    for value in hyp_param_to_vary:        \n",
    "        # change hyperparameter that is being varied\n",
    "        if hyp_param_to_vary == epsilons:\n",
    "            epsilon = value\n",
    "            tr = utils.bp_matrix(tr_batch_size*2, 10)\n",
    "            te = utils.bp_matrix(te_batch_size*2, 10)\n",
    "        elif hyp_param_to_vary == alpha_wds:\n",
    "            alpha_wd = value\n",
    "            tr = utils.bp_matrix(tr_batch_size, 10)\n",
    "            te = utils.bp_matrix(te_batch_size, 10)\n",
    "        else:\n",
    "            alpha_jr = value\n",
    "            tr = utils.bp_matrix(tr_batch_size, 10)\n",
    "            te = utils.bp_matrix(te_batch_size, 10)\n",
    "        \n",
    "        print(f'\\nBeginning training for model: trained_models/{dataset}/training_round_{training_round}/{model_name}')\n",
    "\n",
    "        # instantiate model and optimizer\n",
    "        learning_rate = 0.01\n",
    "        momentum = 0.9\n",
    "        \n",
    "        # net should be instance of LeNet if using MNIST, DDNet if using CIFAR-10\n",
    "        if dataset == 'MNIST':\n",
    "            net = LeNet()\n",
    "        elif dataset == 'CIFAR-10':\n",
    "            net = DDNet()\n",
    "            \n",
    "        optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        lr_decayer = StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "        # make model CUDA enabled and define GPU/device to use\n",
    "        net.cuda()\n",
    "        \n",
    "        # define adversary to train against if needed\n",
    "        adversary = None\n",
    "        if epsilon != 0:\n",
    "            adversary = GradientSignAttack(predict=net, loss_fn=F.cross_entropy, \n",
    "                            eps=epsilon, clip_min=-3., clip_max=3., targeted=False)\n",
    "\n",
    "        # for tracking training progress\n",
    "        train_losses = []\n",
    "        test_losses = []\n",
    "        test_accuracies = []\n",
    "        jacobian_norms = []\n",
    "\n",
    "        for epoch in range(1, n_epochs + 1):\n",
    "            print(f'Epoch #{epoch}')\n",
    "            train()\n",
    "            test()\n",
    "            lr_decayer.step()\n",
    "        \n",
    "        performance[model_name] = (train_losses, test_losses, test_accuracies, jacobian_norms)\n",
    "        torch.save(net.state_dict(), f'trained_models/{dataset}/training_round_{training_round}/{model_name}')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write performance dictionary to text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(f'trained_models/{dataset}/training_round_{training_round}/training_round_{training_round}_performance.txt','w')\n",
    "f.write(str(performance))\n",
    "f.close()\n",
    "\n",
    "# to read dictionary from file:\n",
    "# f = open(f'models/training_round_{training_round}_performance.txt','r')\n",
    "# d = eval(f.read())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
