{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Code assumes the ability to train using a GPU with CUDA.\n",
    "\"\"\"\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from advertorch.attacks import GradientSignAttack, CarliniWagnerL2Attack, PGDAttack\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "\n",
    "# makes default tensor a CUDA tensor so GPU can be used\n",
    "torch.set_default_tensor_type('torch.cuda.FloatTensor')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define data loaders and data preprocessing steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_preprocess = torchvision.transforms.Compose([\n",
    "                        torchvision.transforms.ToTensor(),\n",
    "                        torchvision.transforms.Normalize((0.1307,), (0.3081,))])\n",
    "# the mean of mnist pixel data is .1307 and the stddev is .3081\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "                    torchvision.datasets.MNIST('./data', train=True, download=True,\n",
    "                         transform=data_preprocess), \n",
    "                    batch_size=64, \n",
    "                    shuffle=False)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "                    torchvision.datasets.MNIST('./data', train=False, download=True,\n",
    "                         transform=data_preprocess), \n",
    "                    batch_size=50, \n",
    "                    shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "    \"\"\"MNIST-modified LeNet-5 model.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, kernel_size=5, stride=1, padding=2)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5, stride=1, padding=0)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2)\n",
    "        self.fc1 = nn.Linear(16*5*5, 120)\n",
    "        self.fc1_drop = nn.Dropout(p=.50)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc2_drop = nn.Dropout(p=.50)\n",
    "        self.fc3 = nn.Linear(84,10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(F.relu(self.conv1(x)))\n",
    "        x = self.pool2(F.relu(self.conv2(x)))\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.fc1_drop(F.relu(self.fc1(x)))\n",
    "        x = self.fc2_drop(F.relu(self.fc2(x)))\n",
    "        x = self.fc3(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define adversarial example generation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_adversarial_samples(og_samples, true_labels, adversary, num_per_samp=1):\n",
    "    \"\"\"Create num_per_samp adversarial examples for each sample in\n",
    "    og_samples. Return the generated samples along with corresponding \n",
    "    adv_labels, a tensor containing the adversarial examples' labels.\n",
    "    \"\"\"\n",
    "    adv_samples = []\n",
    "    for i in range(num_per_samp):\n",
    "        adv_samples.append(adversary.perturb(og_samples, true_labels))\n",
    "    adv_samples = torch.cat(adv_samples, 0)\n",
    "    adv_labels = torch.cat([true_labels]*num_per_samp, 0)\n",
    "    return adv_samples, adv_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define my loss function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_loss(output, labels, alpha_wd=0, alpha_jr=0, x=None, bp_mat=None):\n",
    "    \"\"\"Adds terms for L2-regularization and the norm of the input-output \n",
    "    Jacobian to the standard cross-entropy loss function. Check https://arxiv.org/abs/1908.02729\n",
    "    for alpha_wd, alpha_jr suggestions.\n",
    "    \"\"\"\n",
    "    # standard cross-entropy loss base\n",
    "    loss = F.cross_entropy(output, labels)\n",
    "    \n",
    "    # add l2 regularization to loss \n",
    "    if alpha_wd != 0:\n",
    "        l2 = 0\n",
    "        for p in lenet.parameters():\n",
    "            l2 += p.pow(2).sum()\n",
    "        loss = loss + alpha_wd*l2\n",
    "    \n",
    "    # add input-output jacobian regularization formulation\n",
    "    if alpha_jr != 0:\n",
    "        n_outputs = output.shape[1]\n",
    "        batch_size = x.shape[0]\n",
    "        # needed because some edge-case batches are not standard size\n",
    "        if bp_mat.shape[0]/n_outputs != batch_size:     \n",
    "            bp_mat = bp_matrix(batch_size, n_outputs)\n",
    "        x = x.repeat(n_outputs, 1, 1, 1)\n",
    "        # needed so that we can get gradient of output w.r.t input\n",
    "        x = Variable(x, requires_grad=True)\n",
    "        y = lenet(x)\n",
    "        x_grad = torch.autograd.grad(y, x, grad_outputs=bp_mat, create_graph=True)[0]\n",
    "        # get sum of squared values of the gradient values \n",
    "        j = x_grad.pow(2).sum() / batch_size\n",
    "        loss = loss + alpha_jr*j\n",
    "        # needed so gradients don't accumulate in leaf variables when calling loss.backward in train function\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for Jacobian regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_norm_jacobian(x, net, n_outputs):\n",
    "    \"\"\"Returns squared frobenius norm of the input-output Jacobian averaged \n",
    "    over the entire batch of inputs in x.\n",
    "    \"\"\"\n",
    "    batch_size = x.shape[0]\n",
    "    x = x.repeat(n_outputs, 1, 1, 1)\n",
    "    x = Variable(x, requires_grad=True)\n",
    "    # needed so that we can get gradient of output w.r.t input\n",
    "    y = net(x)\n",
    "    x_grad = torch.autograd.grad(y, x, grad_outputs=bp_mat)[0]\n",
    "    j = x_grad.pow(2).sum() / batch_size\n",
    "    return j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bp_matrix(batch_size, n_outputs):\n",
    "    \"\"\"Creates matrix that is used to calculate Jacobian for multiple input \n",
    "    samples at once.\n",
    "    \"\"\"\n",
    "    idx = torch.arange(n_outputs).reshape(n_outputs,1).repeat(1,batch_size).reshape(batch_size*n_outputs,)\n",
    "    return torch.zeros(len(idx), n_outputs).scatter_(1, idx.unsqueeze(1), 1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define train and test functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(alpha_wd, alpha_jr, adversary=None):\n",
    "    lenet.train()\n",
    "    \n",
    "    for batch_idx, (samples, labels) in enumerate(train_loader):\n",
    "        # sends to GPU, i.e. essentially converts from torch.FloatTensor to torch.cuda.FloatTensor\n",
    "        samples, labels = samples.to(device), labels.to(device)\n",
    "        \n",
    "        # expand dataset with adversarial examples if adversary specified\n",
    "        if adversary != None:\n",
    "            adv_samples, adv_labels = generate_adversarial_samples(samples, labels, adversary)  \n",
    "            samples, labels = torch.cat([samples, adv_samples], 0), torch.cat([labels, adv_labels], 0)\n",
    "                \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = lenet(samples)\n",
    "        \n",
    "        loss = my_loss(output, labels, alpha_wd=alpha_wd, alpha_jr=alpha_jr, x=samples, bp_mat=tr)\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % log_interval == 0:\n",
    "            j = avg_norm_jacobian(samples, lenet, output.shape[1])\n",
    "            print(f'\\tLoss: {loss.item():.6f} Average norm of Jacobian: {j:6f}')\n",
    "            train_losses.append(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(alpha_wd, alpha_jr):\n",
    "    lenet.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    for samples, labels in test_loader:\n",
    "        samples, labels = samples.to(device), labels.to(device)\n",
    "        output = lenet(samples)\n",
    "        test_loss += my_loss(output, labels, alpha_wd=alpha_wd, alpha_jr=alpha_jr, x=samples, bp_mat=te).item()\n",
    "        # output is a tensor, .data retrieves its data, max returns the index of the highest valued element\n",
    "        preds = output.data.max(1, keepdim=True)[1]\n",
    "        correct += preds.eq(labels.data.view_as(preds)).sum().item()\n",
    "                \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_accuracy = 100. * float(correct / len(test_loader.dataset))\n",
    "    \n",
    "    print(f'\\tTest set accuracy: ({test_accuracy:.2f}%)')\n",
    "    \n",
    "    test_accuracies.append(test_accuracy)\n",
    "    test_losses.append(test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Beginning training for model: models/ep0_wd0_jr0.01_1\n",
      "Epoch #1\n",
      "\tLoss: 2.300334 Average norm of Jacobian: 0.021065\n",
      "\tLoss: 0.600776 Average norm of Jacobian: 6.331427\n",
      "\tLoss: 0.241412 Average norm of Jacobian: 7.554722\n",
      "\tLoss: 0.331851 Average norm of Jacobian: 7.947834\n",
      "\tLoss: 0.637592 Average norm of Jacobian: 6.903455\n",
      "\tTest set accuracy: (96.69%)\n",
      "Epoch #2\n",
      "\tLoss: 0.298643 Average norm of Jacobian: 6.309241\n",
      "\tLoss: 0.193557 Average norm of Jacobian: 5.886839\n",
      "\tLoss: 0.172586 Average norm of Jacobian: 6.344795\n",
      "\tLoss: 0.223054 Average norm of Jacobian: 6.685603\n",
      "\tLoss: 0.315696 Average norm of Jacobian: 6.273883\n",
      "\tTest set accuracy: (97.65%)\n",
      "Epoch #3\n",
      "\tLoss: 0.198904 Average norm of Jacobian: 5.357544\n",
      "\tLoss: 0.196035 Average norm of Jacobian: 5.312222\n",
      "\tLoss: 0.127478 Average norm of Jacobian: 5.023313\n",
      "\tLoss: 0.219904 Average norm of Jacobian: 5.885437\n",
      "\tLoss: 0.447362 Average norm of Jacobian: 5.653174\n",
      "\tTest set accuracy: (97.66%)\n",
      "Epoch #4\n",
      "\tLoss: 0.165446 Average norm of Jacobian: 5.378593\n",
      "\tLoss: 0.166462 Average norm of Jacobian: 5.291898\n",
      "\tLoss: 0.157217 Average norm of Jacobian: 4.593221\n",
      "\tLoss: 0.122762 Average norm of Jacobian: 4.952654\n",
      "\tLoss: 0.290825 Average norm of Jacobian: 4.916387\n",
      "\tTest set accuracy: (97.69%)\n",
      "Epoch #5\n",
      "\tLoss: 0.121347 Average norm of Jacobian: 4.790895\n",
      "\tLoss: 0.134742 Average norm of Jacobian: 4.608410\n",
      "\tLoss: 0.144935 Average norm of Jacobian: 5.023592\n",
      "\tLoss: 0.137102 Average norm of Jacobian: 4.808279\n",
      "\tLoss: 0.216856 Average norm of Jacobian: 5.483916\n",
      "\tTest set accuracy: (97.58%)\n",
      "Epoch #6\n",
      "\tLoss: 0.176194 Average norm of Jacobian: 5.051809\n",
      "\tLoss: 0.122301 Average norm of Jacobian: 4.755575\n",
      "\tLoss: 0.171426 Average norm of Jacobian: 4.528944\n",
      "\tLoss: 0.186814 Average norm of Jacobian: 5.174582\n",
      "\tLoss: 0.312710 Average norm of Jacobian: 4.647464\n",
      "\tTest set accuracy: (98.38%)\n",
      "Epoch #7\n",
      "\tLoss: 0.120250 Average norm of Jacobian: 4.359938\n",
      "\tLoss: 0.178256 Average norm of Jacobian: 4.374966\n",
      "\tLoss: 0.102484 Average norm of Jacobian: 4.556956\n",
      "\tLoss: 0.215756 Average norm of Jacobian: 4.832659\n",
      "\tLoss: 0.188714 Average norm of Jacobian: 3.649910\n",
      "\tTest set accuracy: (97.80%)\n",
      "Epoch #8\n",
      "\tLoss: 0.141117 Average norm of Jacobian: 4.483152\n",
      "\tLoss: 0.154052 Average norm of Jacobian: 4.688903\n",
      "\tLoss: 0.101888 Average norm of Jacobian: 4.150964\n",
      "\tLoss: 0.153469 Average norm of Jacobian: 4.169991\n",
      "\tLoss: 0.169390 Average norm of Jacobian: 4.022100\n",
      "\tTest set accuracy: (98.00%)\n",
      "Epoch #9\n",
      "\tLoss: 0.134413 Average norm of Jacobian: 4.045561\n",
      "\tLoss: 0.088058 Average norm of Jacobian: 4.279144\n",
      "\tLoss: 0.093678 Average norm of Jacobian: 3.565065\n",
      "\tLoss: 0.171177 Average norm of Jacobian: 4.116683\n",
      "\tLoss: 0.271717 Average norm of Jacobian: 3.972235\n",
      "\tTest set accuracy: (97.79%)\n",
      "Epoch #10\n",
      "\tLoss: 0.138275 Average norm of Jacobian: 3.313198\n",
      "\tLoss: 0.095154 Average norm of Jacobian: 4.078330\n",
      "\tLoss: 0.097458 Average norm of Jacobian: 4.309850\n",
      "\tLoss: 0.134143 Average norm of Jacobian: 4.029134\n",
      "\tLoss: 0.290445 Average norm of Jacobian: 3.859089\n",
      "\tTest set accuracy: (98.62%)\n",
      "Epoch #11\n",
      "\tLoss: 0.094002 Average norm of Jacobian: 3.766758\n",
      "\tLoss: 0.169152 Average norm of Jacobian: 3.796569\n",
      "\tLoss: 0.116538 Average norm of Jacobian: 3.944751\n",
      "\tLoss: 0.125430 Average norm of Jacobian: 3.363575\n",
      "\tLoss: 0.120386 Average norm of Jacobian: 3.639161\n",
      "\tTest set accuracy: (98.83%)\n",
      "Epoch #12\n",
      "\tLoss: 0.105472 Average norm of Jacobian: 3.570094\n",
      "\tLoss: 0.175244 Average norm of Jacobian: 3.443471\n",
      "\tLoss: 0.087340 Average norm of Jacobian: 4.033216\n",
      "\tLoss: 0.160552 Average norm of Jacobian: 3.391899\n",
      "\tLoss: 0.214637 Average norm of Jacobian: 3.436460\n",
      "\tTest set accuracy: (98.81%)\n",
      "Epoch #13\n",
      "\tLoss: 0.093246 Average norm of Jacobian: 3.325532\n",
      "\tLoss: 0.138885 Average norm of Jacobian: 3.607249\n",
      "\tLoss: 0.082289 Average norm of Jacobian: 4.030829\n",
      "\tLoss: 0.103423 Average norm of Jacobian: 3.411284\n",
      "\tLoss: 0.212779 Average norm of Jacobian: 3.833568\n",
      "\tTest set accuracy: (98.80%)\n",
      "Epoch #14\n",
      "\tLoss: 0.084166 Average norm of Jacobian: 3.523397\n",
      "\tLoss: 0.135041 Average norm of Jacobian: 3.629237\n",
      "\tLoss: 0.102191 Average norm of Jacobian: 3.761191\n",
      "\tLoss: 0.103523 Average norm of Jacobian: 3.146038\n",
      "\tLoss: 0.186517 Average norm of Jacobian: 3.514084\n",
      "\tTest set accuracy: (98.83%)\n",
      "Epoch #15\n",
      "\tLoss: 0.090143 Average norm of Jacobian: 3.386829\n",
      "\tLoss: 0.096622 Average norm of Jacobian: 3.676689\n",
      "\tLoss: 0.064026 Average norm of Jacobian: 4.004286\n",
      "\tLoss: 0.131403 Average norm of Jacobian: 3.325855\n",
      "\tLoss: 0.116719 Average norm of Jacobian: 3.685205\n",
      "\tTest set accuracy: (98.89%)\n",
      "Epoch #16\n",
      "\tLoss: 0.118215 Average norm of Jacobian: 3.426567\n",
      "\tLoss: 0.209221 Average norm of Jacobian: 3.592497\n",
      "\tLoss: 0.076733 Average norm of Jacobian: 3.700395\n",
      "\tLoss: 0.114171 Average norm of Jacobian: 3.304823\n",
      "\tLoss: 0.155450 Average norm of Jacobian: 3.473241\n",
      "\tTest set accuracy: (98.79%)\n",
      "Epoch #17\n",
      "\tLoss: 0.126475 Average norm of Jacobian: 3.461038\n",
      "\tLoss: 0.133152 Average norm of Jacobian: 3.608578\n",
      "\tLoss: 0.106923 Average norm of Jacobian: 3.753441\n",
      "\tLoss: 0.124076 Average norm of Jacobian: 3.322327\n",
      "\tLoss: 0.196396 Average norm of Jacobian: 3.664680\n",
      "\tTest set accuracy: (98.82%)\n",
      "Epoch #18\n",
      "\tLoss: 0.127809 Average norm of Jacobian: 3.570962\n",
      "\tLoss: 0.106731 Average norm of Jacobian: 3.480278\n",
      "\tLoss: 0.141693 Average norm of Jacobian: 3.421991\n",
      "\tLoss: 0.122322 Average norm of Jacobian: 3.210711\n",
      "\tLoss: 0.138586 Average norm of Jacobian: 3.818604\n",
      "\tTest set accuracy: (98.86%)\n",
      "Epoch #19\n",
      "\tLoss: 0.103663 Average norm of Jacobian: 3.570645\n",
      "\tLoss: 0.083299 Average norm of Jacobian: 3.636498\n",
      "\tLoss: 0.121871 Average norm of Jacobian: 3.677227\n",
      "\tLoss: 0.100410 Average norm of Jacobian: 3.265236\n",
      "\tLoss: 0.157232 Average norm of Jacobian: 3.311278\n",
      "\tTest set accuracy: (98.81%)\n",
      "Epoch #20\n",
      "\tLoss: 0.100718 Average norm of Jacobian: 3.437274\n",
      "\tLoss: 0.245353 Average norm of Jacobian: 3.421924\n",
      "\tLoss: 0.091337 Average norm of Jacobian: 3.631974\n",
      "\tLoss: 0.150670 Average norm of Jacobian: 2.976155\n",
      "\tLoss: 0.150126 Average norm of Jacobian: 3.610155\n",
      "\tTest set accuracy: (98.91%)\n",
      "Epoch #21\n",
      "\tLoss: 0.088255 Average norm of Jacobian: 3.406424\n",
      "\tLoss: 0.113014 Average norm of Jacobian: 3.605260\n",
      "\tLoss: 0.103193 Average norm of Jacobian: 3.554307\n",
      "\tLoss: 0.122745 Average norm of Jacobian: 3.195150\n",
      "\tLoss: 0.164874 Average norm of Jacobian: 3.724916\n",
      "\tTest set accuracy: (98.91%)\n",
      "Epoch #22\n",
      "\tLoss: 0.071206 Average norm of Jacobian: 3.695403\n",
      "\tLoss: 0.121766 Average norm of Jacobian: 3.957484\n",
      "\tLoss: 0.105084 Average norm of Jacobian: 3.445922\n",
      "\tLoss: 0.131889 Average norm of Jacobian: 3.329896\n",
      "\tLoss: 0.184562 Average norm of Jacobian: 3.564913\n",
      "\tTest set accuracy: (98.88%)\n",
      "Epoch #23\n",
      "\tLoss: 0.116755 Average norm of Jacobian: 3.508298\n",
      "\tLoss: 0.160113 Average norm of Jacobian: 3.574492\n",
      "\tLoss: 0.135058 Average norm of Jacobian: 3.566072\n",
      "\tLoss: 0.113243 Average norm of Jacobian: 3.069621\n",
      "\tLoss: 0.112042 Average norm of Jacobian: 3.386389\n",
      "\tTest set accuracy: (98.89%)\n",
      "Epoch #24\n",
      "\tLoss: 0.136290 Average norm of Jacobian: 3.537767\n",
      "\tLoss: 0.123467 Average norm of Jacobian: 3.639400\n",
      "\tLoss: 0.086848 Average norm of Jacobian: 3.747462\n",
      "\tLoss: 0.141180 Average norm of Jacobian: 3.198732\n",
      "\tLoss: 0.177120 Average norm of Jacobian: 3.358856\n",
      "\tTest set accuracy: (98.88%)\n",
      "Epoch #25\n",
      "\tLoss: 0.084879 Average norm of Jacobian: 3.568902\n",
      "\tLoss: 0.098055 Average norm of Jacobian: 3.900130\n",
      "\tLoss: 0.081726 Average norm of Jacobian: 3.651208\n",
      "\tLoss: 0.140666 Average norm of Jacobian: 3.161991\n",
      "\tLoss: 0.209933 Average norm of Jacobian: 3.578540\n",
      "\tTest set accuracy: (98.92%)\n",
      "Epoch #26\n",
      "\tLoss: 0.099059 Average norm of Jacobian: 3.368800\n",
      "\tLoss: 0.169899 Average norm of Jacobian: 3.539952\n",
      "\tLoss: 0.117189 Average norm of Jacobian: 3.592952\n",
      "\tLoss: 0.122905 Average norm of Jacobian: 2.939838\n",
      "\tLoss: 0.170331 Average norm of Jacobian: 3.712691\n",
      "\tTest set accuracy: (98.89%)\n",
      "Epoch #27\n",
      "\tLoss: 0.069539 Average norm of Jacobian: 3.322643\n",
      "\tLoss: 0.172372 Average norm of Jacobian: 3.686381\n",
      "\tLoss: 0.109368 Average norm of Jacobian: 3.469027\n",
      "\tLoss: 0.141668 Average norm of Jacobian: 3.082379\n",
      "\tLoss: 0.168300 Average norm of Jacobian: 3.401780\n",
      "\tTest set accuracy: (98.88%)\n",
      "Epoch #28\n",
      "\tLoss: 0.132368 Average norm of Jacobian: 3.338154\n",
      "\tLoss: 0.191233 Average norm of Jacobian: 3.387060\n",
      "\tLoss: 0.075314 Average norm of Jacobian: 3.786196\n",
      "\tLoss: 0.127088 Average norm of Jacobian: 3.115012\n",
      "\tLoss: 0.168914 Average norm of Jacobian: 3.465903\n",
      "\tTest set accuracy: (98.90%)\n",
      "Epoch #29\n",
      "\tLoss: 0.111116 Average norm of Jacobian: 3.363948\n",
      "\tLoss: 0.112663 Average norm of Jacobian: 3.621196\n",
      "\tLoss: 0.102711 Average norm of Jacobian: 3.836304\n",
      "\tLoss: 0.137324 Average norm of Jacobian: 3.249456\n",
      "\tLoss: 0.203468 Average norm of Jacobian: 3.632449\n",
      "\tTest set accuracy: (98.88%)\n",
      "Epoch #30\n",
      "\tLoss: 0.066799 Average norm of Jacobian: 3.315985\n",
      "\tLoss: 0.106970 Average norm of Jacobian: 3.524162\n",
      "\tLoss: 0.103223 Average norm of Jacobian: 3.529416\n",
      "\tLoss: 0.129262 Average norm of Jacobian: 3.267184\n",
      "\tLoss: 0.118657 Average norm of Jacobian: 3.649935\n",
      "\tTest set accuracy: (98.90%)\n",
      "\n",
      "Beginning training for model: models/ep0_wd0_jr0.01_1\n",
      "Epoch #1\n",
      "\tLoss: 2.308952 Average norm of Jacobian: 0.019872\n",
      "\tLoss: 0.493248 Average norm of Jacobian: 5.232204\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-ebf1de37376b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Epoch #{epoch}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m             \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha_wd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malpha_wd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha_jr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malpha_jr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madversary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFGSM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m             \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha_wd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malpha_wd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha_jr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malpha_jr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0mlr_decayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-b116041379c7>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(alpha_wd, alpha_jr, adversary)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mlenet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0;31m# sends to GPU, i.e. essentially converts from torch.FloatTensor to torch.cuda.FloatTensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py37/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py37/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/py37/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontainer_abcs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0mtransposed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_collate_err_msg_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py37/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontainer_abcs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0mtransposed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_collate_err_msg_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py37/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# training details\n",
    "torch.manual_seed(1)\n",
    "n_epochs = 30\n",
    "log_interval = 200\n",
    "training_round = 1\n",
    "\n",
    "# varying values for certain hyperparameters to produce models with varying degrees of robustness\n",
    "epsilons = [0, .1, .2, .3, .4, .5, .6, .7]\n",
    "alpha_wds = [0.00005, .000001, .000005, .00001, .00005, .0001, .0005, .001]\n",
    "alpha_jrs = [.00000001, .0000001, .000001, .00001, .0001, .001, .01, .1]\n",
    "\n",
    "# these needed so that calculating jacobian across a batch of inputs is parallelizable\n",
    "tr = bp_matrix(64, 10)\n",
    "te = bp_matrix(50, 10)\n",
    "\n",
    "# dictionary to record each model's training/testing stats\n",
    "performance = {}\n",
    "\n",
    "for hyp_param_to_vary in [alpha_jrs, epsilons, alpha_wds]:\n",
    "    epsilon = 0\n",
    "    alpha_wd = 0\n",
    "    alpha_jr = 0\n",
    "    \n",
    "    for value in hyp_param_to_vary:        \n",
    "        # change hyperparameter that is being varied\n",
    "        if hyp_param_to_vary == epsilons:\n",
    "            epsilon = value\n",
    "        elif hyp_param_to_vary == alpha_wds:\n",
    "            alpha_wd = value\n",
    "        else:\n",
    "            alpha_jr = value\n",
    "        \n",
    "        print(f'\\nBeginning training for model: models/ep{epsilon}_wd{alpha_wd}_jr{alpha_jr}_{training_round}')\n",
    "\n",
    "        # instantiate model and optimizer\n",
    "        learning_rate = 0.01\n",
    "        momentum = 0.9\n",
    "        lenet = LeNet()\n",
    "        optimizer = optim.SGD(lenet.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        lr_decayer = StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "        # make model CUDA enabled and define GPU/device to use\n",
    "        lenet.cuda()\n",
    "        device = 0\n",
    "        \n",
    "        # define adversary to train against if needed\n",
    "        FGSM = None\n",
    "        if epsilon != 0:\n",
    "            FGSM = GradientSignAttack(predict=lenet, loss_fn=F.cross_entropy, \n",
    "                            eps=epsilon, clip_min=-3., clip_max=3., targeted=False)\n",
    "\n",
    "        # for tracking training progress\n",
    "        train_losses = []\n",
    "        test_losses = []\n",
    "        test_accuracies = []\n",
    "\n",
    "        for epoch in range(1, n_epochs + 1):\n",
    "            print(f'Epoch #{epoch}')\n",
    "            train(alpha_wd=alpha_wd, alpha_jr=alpha_jr, adversary=FGSM)\n",
    "            test(alpha_wd=alpha_wd, alpha_jr=alpha_jr)\n",
    "            lr_decayer.step()\n",
    "        \n",
    "        performance[f'ep{epsilon}_wd{alpha_wd}_jr{alpha_jr}'] = (train_losses, test_losses, test_accuracies)\n",
    "        torch.save(lenet.state_dict(), f'models/ep{epsilon}_wd{alpha_wd}_jr{alpha_jr}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write performance dictionary to text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(f'models/training_round_{training_round}_performance.txt','w')\n",
    "f.write(str(performance))\n",
    "f.close()\n",
    "\n",
    "# to read dictionary from file:\n",
    "# f = open(f'models/training_round_{training_round}_performance.txt','r')\n",
    "# d = eval(f.read())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
